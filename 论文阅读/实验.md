# To Do

1. 现在训练会出现 `win_size exceeds image extent. Either ensure that your images are at least 7x7;`错误，将训练跑通
2. 修改网络结构

# diffusion model

E5108: Error executing lua: ...im/lua/lazyvim/plugins/extras/coding/native_snippets.lua:20: attempt to index field 'snippet' (a nil value)

PSNR是一种衡量图像重建质量的指标，通过计算原始图像和失真图像之间的最大可能信号值与实际信号误差的比率来评估图像质量

SSIM是一种更加复杂的图像质量评估指标，用于衡量两幅图像的结构相似性。与PSNR不同，SSIM考虑了图像的亮度、对比度和结构信息，更加接近人类的视觉感知特性。

LPIPS（Learned Perceptual Image Patch Similarity）指标用来衡量两幅图像之间的感知相似性。不同于传统的图像相似性指标，如PSNR（峰值信噪比）或SSIM（结构相似性指数），LPIPS更加关注图像在人类视觉感知上的相似性。

CLIP-score：它可以接收一张图片和一段文字作为输入，然后计算它们之间的相似度。

- 使用clip-score进行text2img关联度计算，对生成的图片和text文本有以下要求：

  1. 生成的图片必须是png或者jpg 格式
  2. 每一个prompt单独写在一个纯文本文件内，以 .txt结尾。
  3. 图片和文本文件必须存放在两个目录内。
  4. 图片的名称和文本文件的名称必须要一一对应，比如有 img_dir/cat.jpg，就要有text_dir/cat.txt

- 脚本的使用：后面加上图像目录的路径和prompt目录的路径，注意，不需要具体到文件，它会自动匹配两个目录中文件名相同的文件

  ```bash
  python clip-score/clip_score.py path/to/image path/to/text
  ```

  如果存在可用的GPU设备，此程序会自动执行在GPU设备上。 如果你想制定某个GPU 设备，通过 *--device cuda:N* 去指定。 *--device cpu ,* 表示执行在CPU上。

  > clip-score/clip_score.py 目前限制 prompt最大长度为77个。如果超过77，默认会挂掉。
  > 可通过修改[https://github.com/Taited/clip-score/blob/dbfa0cabd7bfe12f84899786731c35aa62baab4a/src/clip_score/clip_score.py#L131](https://link.zhihu.com/?target=https%3A//github.com/Taited/clip-score/blob/dbfa0cabd7bfe12f84899786731c35aa62baab4a/src/clip_score/clip_score.py%23L131)，修改为data = self.tokenizer(data，*truncate=True*).squeeze()，把超过77的部分截断。

p2p和diffedit的输入图片存储在input_img目录中，输出图片存储在output_img目录中，输出图片对应的prompt存储在prompt目录中

计算某个模型的原图和修改后图片的PSNR、SSIM和LIPIS指标的命令：

```
python calculate_metrics.py prompt-to-prompt/input_img/ prompt-to-prompt/output_img/
python calculate_metrics.py DiffEdit-stable-diffusion/input_img/ DiffEdit-stable-diffusion/output_img/
```

计算某个模型的修改后图片和prompt的相似度的命令：

```
python clip-score/clip_score.py prompt-to-prompt/output_img/ prompt-to-prompt/prompt
python clip-score/clip_score.py DiffEdit-stable-diffusion/output_img/ DiffEdit-stable-diffusion/prompt
```

如果报错 `RuntimeError: cuDNN error: CUDNN_STATUS_NOT_INITIALIZED`或者 `RuntimeError: GET was unable to find an engine to execute this computation`说明运行clip脚本或者PSNR脚本的GPU显存不足

**pnp复现**

```
conda env create -f environment.yaml
conda activate pnp-diffusion
```

将从huggingface上下的stable-diffusion的权重软连接到pnp的目录下

```
mkdir -p models/ldm/stable-diffusion-v1/
ln -s <path/to/model.ckpt> models/ldm/stable-diffusion-v1/model.ckpt 
```

这里我的 `<path/to/model.ckpt>`是 `/home/libo/DiffEdit-stable-diffusion/models/ldm/stable-diffusion-v1/model.ckpt`

特征提取：生成图片的特征提取的config在 `configs/pnp/feature-extraction-generated.yaml`中，真实图片的特征提取的config在 `configs/pnp/feature-extraction-real.yaml`中，里面是提取特征的参数。设置好参数之后，使用下面的命令来进行特征提取

```
python run_features_extraction.py --config <extraction_config_path>
python run_features_extraction.py --config configs/pnp/feature-extraction-generated.yaml 
python run_features_extraction.py --config configs/pnp/feature-extraction-real.yaml 
```

提取完后结果保存在 `experiments/<source_experiment_name>`目录下，`<source_experiment_name>`由config指定，该目录的结构如下

```
- <source_experiment_name>
    - feature_maps         # contains the extracted features
    - predicted_samples    # predicted clean images for each sampling timestep
    - samples              # contains the generated/inverted image
    - translations         # PnP translation results
    - z_enc.pt             # the initial noisy latent code
    - args.json            # the config arguments of the experiment
```

提取完之后运行下面的命令，生成图片的config在 `configs/pnp/pnp-generated.yaml`中，真实图片的config在 `configs/pnp/pnp-real.yaml`中

```
python run_pnp.py --config <pnp_config_path>
python run_pnp.py --config configs/pnp/pnp-generated.yaml
python run_pnp.py --config configs/pnp/pnp-real.yaml
```

**InstructPix2Pix复现**

```
conda env create -f environment.yaml
conda activate ip2p
bash scripts/download_checkpoints.sh
```

# mri

**MINet**（Multi-Contrast MRI Super-Resolution via a Multi-Stage Integration Network）：多模态超分，使用高分辨率的T1WI来辅助超分低分辨率的T2WI

- 超分效果强于EDSR、SMORE、和专用mri的超分方法（Multicontrast super-resolution mri through a progressive network）、（Simultaneous singleand multi-contrast super-resolution for brain mri images based on a convolutional neural network）。前面两个是单模态的超分方法，后面两个是多模态的超分方法

  ![image-20240223153125658](实验.assets/image-20240223153125658.png)

T2Net（Task Transformer Network for Joint MRI Reconstruction and Super-Resolution）：mri联合超分重建

- 此网络主要买点是可以对既有伪影又是低分辨率的mri图片同时进行重建（去伪影）和超分（提高分辨率），**因此他比较的网络不是纯超分的网络，而是将重建和超分的网络顺序拼接在一起**，先进行重建，再进行超分。重建对应6倍笛卡尔加速，超分对应2倍和4倍的scale。
-  Com-A: ADMMNet MGLRL, Com-B: ADMMNet-Lyu *et al.*, Com-C: MICCAN-MGLRL, and Com D: MICCAN-Lyu *et al.*
- ![image-20240223155022934](实验.assets/image-20240223155022934.png)

Cross-Modality High-Frequency Transformer for MR Image Super-Resolution：此工作也是进行多模态超分，**使用transformer对低分辨率的T2WI图像进行超分，并且使用低分辨率的T2WI的梯度图和高分辨率的T1WI的梯度图作为引导**

- 按照他的数据，它的超分效果强于一众网络，包括上面的T2Net（T2Net既然可以同时进行超分和重建，那么它当然也可以用于纯超分）和MINet，并且强于目前的mri超分的sota MTrans（Multi-Modal Transformer for Accelerated MR Imaging）  

  ![image-20240223162743908](实验.assets/image-20240223162743908.png)

此文章表示：最好的自然图片超分方法：swinIR（Image restoration using swin transformer）；最好的基于参考图片超分的方法MASA-SR（Matching Acceleration and Spatial Adaptation for Reference-Based Image Super-Resolution）

MTrans（Multi-Modal Transformer for Accelerated MR Imaging） ：本文的多模态transformer可以对T2WI进行重建和超分，也可以对FS-PDWI进行重建和超分，分别使用T1WI和PDWI来引导。

# T2Net（IXI数据集预处理）

IXI数据集有大概五六百个多模态的mri volume，包括三种模态，T2WI、T1WI和PD-WI。IXI数据集中，T2 和 PD 加权图像的原始 HR 图像大小均为 256 × 256。通过向下采样和零填充，生成 2 倍、3 倍和 4 倍向下采样的 T2 加权 LR 图像

IXI数据集原始的格式是nil格式，我们将其先转化成h5格式，在h5文件中创建dataset名为data。注意，后面加载h5文件时访问的名字要和这里dataset的名字一致，所以下面将h5文件中的切片提取出来保存成mat格式时名字也是data

![image-20240423231844201](实验.assets/image-20240423231844201.png)

每一个h5文件都由100多个切片组成的，遍历所有h5文件，在每个文件中遍历所有切片，将每个切片单独保存为mat格式的文件，这些文件都是原始的高分辨率的图像，而低分辨率的图像并没有提前预处理出来，而是在运行时一边处理图像一边运行模型

![image-20240423232019211](实验.assets/image-20240423232019211.png)

- 多线程处理：

  ```python
  def save_mat(h5path, h5f, matpath):
      fname = os.path.splitext(h5f)[0]
      print(fname)
      with h5py.File(os.path.join(h5path, h5f), 'r') as f:
          slices = f['data'].shape[2]
          for slice in range(slices):
              img = f['data'][..., slice]
              matfile = os.path.join(matpath, f"{fname}-{slice:03d}.mat")
              sio.savemat(matfile, {'img': img})
  
  def convert(h5path, matpath):
      h5files = os.listdir(h5path)
      os.makedirs(matpath, exist_ok=True)
      with ThreadPoolExecutor() as executor:
          # 将处理单个h5文件（将h5文件转换成100多个mat文件）作为一个任务，提交给线程池，每个线程负责一个任务
          futures = [executor.submit(save_mat, h5path, h5f, matpath) for h5f in h5files]
          for _ in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):
              pass
  
  def main(root):
      train_dir = os.path.join(root, 'train')
      val_dir = os.path.join(root, 'val')
      test_dir = os.path.join(root, 'test')
  
      matpath = root.replace('/h5', '/mat')
      os.makedirs(matpath, exist_ok=True)
  
      convert(train_dir, os.path.join(matpath, 'train'))
      convert(val_dir, os.path.join(matpath, 'val'))
      convert(test_dir, os.path.join(matpath, 'test'))
  ```

  

然后在运行时，重载dataloader，init函数负责创建validation或者test的测试集，主要是从每个h5文件（volume）的100多个切片中选择20个切片作为测试集，选择时排除了每个volume的前面的一些切片，因为额叶切片比其他切片要嘈杂，导致他们的分布有些不同。ids用来保存每个切片的h5名字和在原h5文件中对应的id，LR_slice_files用来保存新挑选出来的切片保存在本地的路径

![image-20240423185235692](实验.assets/image-20240423185235692.png)

从dataloader中获取数据时调用的是 `__getitem__`函数，

1. 根据传入的参数i，在ids中找到对应的切片的h5名字和在原h5文件中对应的id，将他们拼接起来就能得到该切片在mat目录下的路径，然后将该mat格式的切片读取出来，形状是256 * 256 ，即整个网络的target HR
2. 然后对该切片进行fft得到对应的k空间图像（256 * 256），对其进行截断成128 * 128的k空间图像T，
3. 然后导入它提供的128 * 128的掩码，对T进行下采样，再进行逆傅里叶变换得到网络输入的即包含伪影又低分辨率的图像LR，
4. 对T进行逆傅里叶变换得到重建分支的target（不包含伪影的低分辨率图像LR_ori）

然后将得到的HR、LR和LR_ori、kspace和target返回

凑够batch_size个数据后，就调用training_step或者validation_step，他们再将LR作为参数调用UnetModule的forward方法，计算出重建分支的输出和整个网络的输出，再用二者联合计算出loss

![image-20240423221508695](实验.assets/image-20240423221508695.png)

具体的步骤：

![image-20240423223035647](实验.assets/image-20240423223035647.png)

# Dual-Arbnet

数据目录分成两个目录，一个目录是输入的测试的mri，一个目录是ref的mri。还有一个txt文件，用来将这两个目录下的mri文件名一一对应

![image-20240709145912282](实验.assets/image-20240709145912282.png)

![image-20240709150055599](实验.assets/image-20240709150055599.png)

初始化dataset的流程：

main函数：![image-20240709151034421](实验.assets/image-20240709151034421.png)

data目录下 `__init__.py`文件

![image-20240709151346758](实验.assets/image-20240709151346758.png)

data目录下 `multiscaleRefMRIdata.py`文件，进入 `RefRIData`类中的 `__init__`函数，开始构造dataset

首先根据reflist文件构造出一个字典，key是lr文件的名字，val是对应的ref文件的路径

![image-20240709152324665](实验.assets/image-20240709152324665.png)

dataset中 `__getitem__`时会根据传入的mri的idx，找到目录中对应的hr和ref_hr，再根据指定的scale，将它们进行截断得到lr和ref_lr。然后将lr和ref_lr在通道维度上拼接得到lr_cat，再将lr_cat和hr、ref_hr获取其中的patch（比如32 x 32）。最后getitem返回的是

![image-20240710161108472](实验.assets/image-20240710161108472.png)

下面是数据在整个过程中shape的变化：

![image-20240711112544861](实验.assets/image-20240711112544861.png)

![image-20240711113125537](实验.assets/image-20240711113125537.png)

![image-20240711114119001](实验.assets/image-20240711114119001.png)

![image-20240711114243783](实验.assets/image-20240711114243783.png)

![image-20240711114720783](实验.assets/image-20240711114720783.png)

![image-20240711162136596](实验.assets/image-20240711162136596.png)

<img src="实验.assets/image-20240711162609996.png" alt="image-20240711162609996" style="zoom:200%;" />

<img src="实验.assets/image-20240711162915364.png" alt="image-20240711162915364" style="zoom:200%;" />

# 如何实现冻结网络的部分参数

对于你不希望更新的网络部分，**将这些部分的参数的 `requires_grad` 属性设置为 `False`**（注意，在前向推理时使用 `with torch.no_grad():` 是为了在计算过程中不计算梯度和不进行自动微分，但这仅对前向计算的部分有效，不会影响反向传播的过程）

![image-20240829194808177](实验.assets/image-20240829194808177.png)

在初始化优化器时，确保只将需要更新的参数传递给优化器。可以使用 `filter` 函数来过滤掉那些 `requires_grad = False` 的参数，**这里通常库函数已经帮我们写好了**。例如：

![image-20240829194916187](实验.assets/image-20240829194916187.png)

# trick

![image-20240326224022981](实验.assets/image-20240326224022981.png)

![image-20240326225432361](实验.assets/image-20240326225432361.png)

![image-20240326225404913](实验.assets/image-20240326225404913.png)

![image-20240326225417597](实验.assets/image-20240326225417597.png)

![image-20240326225809390](实验.assets/image-20240326225809390.png)

![image-20240326230121165](实验.assets/image-20240326230121165.png)

![image-20240326230412811](实验.assets/image-20240326230412811.png)

下笔：

https://www.bilibili.com/video/BV1oP411x7H3/?spm_id_from=333.999.0.0&vd_source=e282c92468149e878c5fe148b7a85f11

引用模块如何洗稿：

<img src="实验.assets/image-20240409171628078.png" alt="image-20240409171628078" style="zoom:67%;" />

用别人论文的模块a，对其进行一些修改（比如加上另外一篇论文的模块c）当成自己的模块A，然后描述的时候说我们设计了一个A，而不是说我们引用了别人的a；后者会让审稿人认为a完全是别人的东西，而前者可以让不熟悉的审稿人认为A完全是自己的东西（关键是参考引用的模型足够隐蔽，没那么火）

- 正常来讲对a进行修改需要对修改进行消融实验，但是由于A已经和a不一样了，我们可以说A是我们自己设计的，所以只需要对整个A进行消融实验了

实际上整个模型都可以这么来做，比如别人的基准模型A+模块B+模块C=D，我们对三者都进行稍微的修改之后就可以说我们设计了一个网络D，里面包括了A' B' C'。（名字要改，最多在相关工作提一嘴）

- 删了一点东西可以叫 轻量化

换领域不需要修改模型也可以，但得保证你是领域第一人（直接去用顶刊transformer的变体模型，我不去修改别人的模型结构，但是我在别人没用应用过的领域并且用自己的数据）

![image-20240410103828279](实验.assets/image-20240410103828279.png)

![image-20240410105111050](实验.assets/image-20240410105111050.png)

如果比不过顶刊的最差的（论文中声称的结果），那么就复现顶刊的代码，在不同的环境下跑，比如在不同的参数、显卡、随机种子下跑，直到比得过顶刊最差的为止

- 如果审稿人指出原模型的指标与论文里对比的指标不符，你就说这是你复现的结果啊，你有证据的啊，论文复现后的结果就是有可能不一样

![image-20240410143155264](实验.assets/image-20240410143155264.png)

![image-20240515151852522](实验.assets/image-20240515151852522.png)

相似领域找灵感，边试边改勇投稿。

![image-20240515153537523](实验.assets/image-20240515153537523.png)

![image-20240515160319161](实验.assets/image-20240515160319161.png)

![image-20240515160726285](实验.assets/image-20240515160726285.png)
