# CMU 15-445学习笔记

# SQL笔记

在关系数据库中，一张表中的每一行数据被称为一条记录。一条记录就是由多个字段组成的。

主键是关系表中记录的唯一标识。

主键也不应该允许`NULL`。

关系数据库实际上还允许通过多个字段唯一标识记录，即两个或更多的字段都设置为主键，这种主键被称为联合主键。

对于联合主键，允许一列有重复，只要不是所有主键列都重复即可：

| id_num | id_type | other columns... |
| :----- | :------ | :--------------- |
| 1      | A       | ...              |
| 2      | A       | ...              |
| 2      | B       | ...              |

如果我们把上述表的`id_num`和`id_type`这两列作为联合主键，那么上面的3条记录都是允许的，因为没有两列主键组合起来是相同的。

外键：一个表的某一个属性是另一个表的属性

# lecture 1 关系模型

## 课程大纲

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20220122205954688.png" alt="image-20220122205954688" style="zoom: 50%;" />

从上层的⻆度来讲，这是⼀个⾯向硬盘的数据管理系统（disk oriented data management system），我们假设数据库在磁盘上，然后将数据写到磁盘上。

- 我们会以上层的⻆度来讨论关系型数据库是什么 ，
- 如何存储它们
- 如何在它们之上进⾏查询
- 如何在它们之上执行事务管理，
- 如果遇上冲突或者我们需要重启系统时该如何去恢复它们 
- 接着会讨论一些高级主题，例如分布式数据库，或者⼀些其他类型的数据库以及关系型数据库的⼀些扩展 
- 然后我们会去讨论如何对它们的规模进行扩展以及如何在云环境运⾏它们 

我们需要从头开始⼀点⼀点的来构建出⼀个功能完整的**数据库存储管理器**（database storage manager），注意！是存储管理器（storage manager）而不是数据库系统（database system），因为在这⾥⾯，我们⽆法运行SQL语句，也没有查询解析器。但我们能够通过提供给我们的⼿写代码来执⾏查询 ，因此，这要远⽐简单的键值存储来的复杂得多，但它并不是⼀个功能完善的系统。

## 数据库管理系统

应用程序需要使用数据库保存用户的数据，比如Word需要把用户文档保存起来，以便下次继续编辑或者拷贝到另一台电脑。

要保存用户的数据，一个最简单的方法是把用户数据写入文件。例如，要保存一组音乐家的信息，可以写入一个CSV文件：

![image-20210902184648727](https://raw.githubusercontent.com/BoL0150/image2/master/image-20210902184648727.png)

我们可以用如下代码遍历文件中的每一行找到`Ice Cube` 出道的年份：

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/10803273-cd3caeefec687708.png" alt="img" style="zoom:67%;" />

所以数据库是数据的集合

问题：

- DATA INTEGRITY（数据完整性）：
  - 我们该如何确保每个artist在album中的位置相同？
  - 如果有人在出道日期上写了string怎么办？
  - 我们要如何存储多个音乐家合作发布的专辑呢？

-  IMPLEMENTATION：
  - 如果有十几亿个数据，如何快速找到一个特定的条目
  - 有一个新的APP 想用同一个DB，不应该重复造轮子
  - 2个线程同时写同一个文件怎么办？

- DURABILITY（持久性）：
  - 如果在我们的程序更新记录时机器崩溃怎么办？
  - 如果我们想在多台机器上复制数据库以获得高可用性，该怎么办？

这就引出了我们数据库管理系统（DBMS），DBMS是允许应用程序存储和分析数据库中的信息的软件。通用DBMS可以在允许程序在无序关心数据库的底层实现的情况下，对数据库中的信息进行存储和分析，它是一种能够被多种应用所复用的软件，不需要重复造轮子。DBMS可以允许应用程序来对数据库进行定义，创建，查询，更新和管理。

## 关系模型

关系模型有三点关键：

- 将数据库存储在简单的数据结构（关系）中

  Relational Terminology：

  - **Database**: Set of named Relations
  - **Relation** (**Table**):
    - **Schema**: description (“metadata”)
    - **Instance**: set of data satisfying the schema
  - **Attribute** (Column, Field)
  - **Tuple** (Record, Row)

  下面是一个relation（table），schema有三列（每一列分别有名字和类型），Instance有三行或者三条记录，每一行都是一个Tuple，每一列都是一个Attribute

  <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20210903112916505.png" alt="image-20210903112916505" style="zoom:67%;" />

  schema是固定的（它可以改变，但通常不会改变），schema中每一个attribute的名字都是**唯一**的，并且所有attribute的类型都是**原子的（即不可再分的）**，所以attribute不能是collection类型或list类型。Instance会经常改变，**由多组tuple（或行）组成**。

- 通过高级语言访问数据。

- 物理存储策略取决于数据库管理系统的实现

  我们通过这些⾼级结构作为关系来定义我们的数据库，但实际上如何存储这些关系则取决于数据库系统的实现

数据模型是用于描述数据库中数据的概念的集合。关系模型是数据模型的一个例子。还有很多其他的数据模型：

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20211015163303398.png" alt="image-20211015163303398" style="zoom: 50%;" />



SQL包含两个sublanguages:

- DDL – Data Definition Language
  - Define and modify schema

- DML – Data Manipulation Language
  - 操作数据库中的实际数据，Queries can be written intuitively.

DBMS 负责对SQL语句进行求值

- Choose and run algorithms for declarative queries

  - Choice of algorithm must not affect query answer.

Primary Key column(s)

- Provides a unique “lookup key” for the relation


- Cannot have any duplicate values


- **Can be made up of >1 column**
  - E.g. (firstname, lastname)

![image-20210904114529326](https://raw.githubusercontent.com/BoL0150/image2/master/image-20210904114529326.png)

外键没有指定引用Sailors和Boats中的哪个键，默认为主键。

## 关系代数

语法：

![image-20220122202746896](https://raw.githubusercontent.com/BoL0150/image2/master/image-20220122202746896.png)

predicate（谓词）：筛选的条件

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20210904172836916.png" alt="image-20210904172836916" style="zoom: 50%;" />

### SELECT

此符号相当于是select 所有的列，即select *

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20210904172636179.png" alt="image-20210904172636179" style="zoom: 67%;" />

### projection

![image-20230213130236241](https://raw.githubusercontent.com/BoL0150/image2/master/image-20230213130236241.png)

投影相当于是从R表中select指定的属性（A1，A2，An）生成一个新表

projection操作还可以将新的值计算出来，将原来的表映射成一张新的表：

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20210904173104244.png" alt="image-20210904173104244" style="zoom:67%;" />

选择查询是挑选出符合条件的行，投影查询是选择想要的列

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230213134604515.png" alt="image-20230213134604515" style="zoom:50%;" />

![image-20230213175803629](https://raw.githubusercontent.com/BoL0150/image2/master/image-20230213175803629.png)



### UNION

![image-20230213130530992](https://raw.githubusercontent.com/BoL0150/image2/master/image-20230213130530992.png)

UNION是把两个表中的所有tuple合到一张表中

在SQL语句中UNION是去重的，UNION ALL是不去重的，在下图中就有两条a3

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20210904173324110.png" alt="image-20210904173324110" style="zoom: 50%;" />

### intersection

把两个表中的所有tuple取交集

![image-20230213131002668](https://raw.githubusercontent.com/BoL0150/image2/master/image-20230213131002668.png)

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20210904173404637.png" alt="image-20210904173404637" style="zoom: 50%;" />

### difference

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230213131049450.png" alt="image-20230213131049450" style="zoom:67%;" />

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20210904173437807.png" alt="image-20210904173437807" style="zoom: 50%;" />

### product

笛卡尔积，把两张表中的所有tuple拼起来

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20210904173918270.png" alt="image-20210904173918270" style="zoom:50%;" />

### join

![image-20230213131457969](https://raw.githubusercontent.com/BoL0150/image2/master/image-20230213131457969.png)

如果join符号左边出现两个横杠就是left join，如果右边就是right join，如果两边都有就是out join

**在两张表的笛卡尔积后得到的那张大表中再次选取一些符合我们条件的元组**

join的实现方式是：遍历R表中的tuple，对每一条tuple（记为tupleR)，遍历一次S表，选出S表中条件符合的tuple（记为tupleS），将tupleR和tupleS拼成一条tuple。最后将所有的这些拼成的tuple构成最终join的结果

![image-20230213140258218](https://raw.githubusercontent.com/BoL0150/image2/master/image-20230213140258218.png)

![image-20230213140321954](https://raw.githubusercontent.com/BoL0150/image2/master/image-20230213140321954.png)

这里我们发现了等值连接的一个缺点，R.B和S.B属性是相等的，而我们只需要其中一列就可以，所以R.B和S.B属性只需要保留任意一列就可以了。

**自然连接是一种特殊的等值连接**，自然连接会自动找到相同的属性，并且默认条件就是相同属性的值相同。自然连接去掉了等值连接中相等的属性。

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230213140804232.png" alt="image-20230213140804232" style="zoom:50%;" />

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20210904174353382.png" alt="image-20210904174353382" style="zoom:67%;" />

在where后面加上条件，就是隐式的inner join

![image-20230213180516351](https://raw.githubusercontent.com/BoL0150/image2/master/image-20230213180516351.png)

join和product的sql语法区别：

- join：`select a.name from a,b where a.id = b.id`或 `select * from a,b on a.id = b.id`
- product: 笛卡尔积就是不指定连接谓词的join`select * form a,b`

在做等值连接时由于彼此之间没有对应的元组（彼此之间特有的元组，我有的你没有，或者你有的我没有，这种情况肯定不会相等），在自然连接和等值连接的时候都会被丢弃，这种连接叫做内连接。

而有时候我们需要保留一张表中这种特有的元组，这些元组不能被丢弃，所以需要使用与内连接相反的连接——外连接来解决特有的元组被丢弃的问题。

外连接：把R表和S表被丢弃的元组捡了回来，并且在最终连接的表中没有的值用NULL替代，

- 左外连接：**只保留左边表中的特有元组**，**右边表的特有元组仍然丢弃**
- 右外连接：**只保留右边表中的特有元组**，**左边表的特有元组仍然丢弃**

![image-20230213141304020](https://raw.githubusercontent.com/BoL0150/image2/master/image-20230213141304020.png)



**关系代数仍然在高层次上定义了如何计算一个查询，对查询的具体过程进行了描述**。比如先进行join，再select需要的tuple；和先从一个表select tuple，再拿select的结果进行join，这两种以不同顺序书写的关系代数虽然得到的结果是一样的，但是查询的进行方式是不同的，复杂度也是不一样的。

![](https://raw.githubusercontent.com/BoL0150/image2/master/image-20230213132555034.png)

所以说关系代数是过程语言，而SQL是非过程语言，也叫声明式语言，因为sql不告诉数据库应该以什么方式查询，只告诉数据库期望的结果，查询的过程由数据库自己决定

![image-20230213133038169](https://raw.githubusercontent.com/BoL0150/image2/master/image-20230213133038169.png)

### AGGREGATE

![image-20230213180720481](https://raw.githubusercontent.com/BoL0150/image2/master/image-20230213180720481.png)

aggregate只能对select的输出结果使用。

count(login)是计算login属性（除了null之外）一共有多少行，count(*)是计算整张表一共有多少行

![image-20230213181027833](https://raw.githubusercontent.com/BoL0150/image2/master/image-20230213181027833.png)

 如果select中使用了aggregate就不能输出其他的任何列，只能输出aggregate。因为aggregate输出的是一个数，如果要再输出表中的另外一列是无法匹配的。

![image-20230213181845521](https://raw.githubusercontent.com/BoL0150/image2/master/image-20230213181845521.png)

解决该问题可以使用group by：group by先将表分组，再对每一组进行aggregate函数，由于一组中的分组字段都相同，所以**此时可以同时输出aggregate函数和分组字段，但是仍然不能输出除了分组字段之外的其他任何列**

![image-20230213182021168](https://raw.githubusercontent.com/BoL0150/image2/master/image-20230213182021168.png)

所以，**在select 语句中出现的非aggregate的字段一定要出现在group by语句中**！

如果**想对聚集之后的结果再进行筛选，需要使用Having语句**

![image-20230213182702085](https://raw.githubusercontent.com/BoL0150/image2/master/image-20230213182702085.png)

select语句的执行顺序是先从from按顺序执行到group by，然后再执行select，执行完select之后再执行Having，以及之后的语句。

LIKE语句跟在where后面，用于对字符串模糊查询：

![image-20230213183113626](https://raw.githubusercontent.com/BoL0150/image2/master/image-20230213183113626.png)

字符串拼接：

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230213183218901.png" alt="image-20230213183218901" style="zoom: 50%;" />

MySQL可以直接用select的输出结果重定向建立一张新表：

![image-20230213183505128](https://raw.githubusercontent.com/BoL0150/image2/master/image-20230213183505128.png)

可以使用order by对输出的结果进行控制

![image-20230213183647263](https://raw.githubusercontent.com/BoL0150/image2/master/image-20230213183647263.png)

如果有多个order by条件，比如下图，就是表示先按grade升序，对grade相同的tuple再用sid升序

![image-20230213183740731](https://raw.githubusercontent.com/BoL0150/image2/master/image-20230213183740731.png)

LIMIT限制输出的tuple数量

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230213184025315.png" alt="image-20230213184025315" style="zoom:50%;" />

子查询：

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230213184300951.png" alt="image-20230213184259280" style="zoom:50%;" />

子查询中常用的语句：

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230213184420808.png" alt="image-20230213184420808" style="zoom:50%;" />

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230213185224430.png" alt="image-20230213185224430" style="zoom: 50%;" />

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230213190207154.png" alt="image-20230213190207154" style="zoom:50%;" />

# lecture 3 Database Storage Part I

我们的数据库的主要存储位置是在非易失性的磁盘中，这种软件被称为⾯向磁盘型数据库系统，这就意味着，每次我们执⾏查询时，所要访问的数据都不在内存中，我们要进⼊磁盘，并取得该数据。

有的数据库是面向内存的，比如redis，这种数据库的特点是：容量小，速度快，断点后数据会丢失

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20211015183607839.png" alt="image-20211015183607839" style="zoom: 50%;" />

这条线以上的是**易失性存储**

- 这意味着当你供电完全断开时，这些数据就不会持续存在 ，当你拔掉你计算机的电源时，所有在DRAM以及CPU缓存中的东⻄都会被擦除
- 支持具有字节可寻址位置的快速随机访问。这意味着程序可以跳转到任何字节地址，并获得那里的数据
-  通常被称为“内存”（memory）

这条分割线以下的就是**⾮易失性存储**

- 它不需要稳定的能量就能持久存储其中保存的所有内容
- 它是块/页面可寻址的。
  - 我们无法获取某一个精确的字节，为了读取特定偏移量下的值，程序首先必须将整个块加载到内存中，然后再解析这个块。
  - 如果要修改某一个字节，也需要将所在的块都读入到内存中，然后对这个块修改后，再写回到磁盘中。
- 非易失性存储更擅长**顺序**访问（同时读取多个连续的数据块）（想象一下机械硬盘旋转）

我们的DBMS的组件统管理了数据从非易失性存储到易失性存储的移动。

因为系统不能直接在磁盘上操作数据。我们将**关注如何隐藏磁盘的延迟**，而不是关注使用寄存器和缓存的优化，因为从磁盘获取数据非常慢。

我们的⽬标是给应⽤程序提供⼀种错觉，即我们能提供足够的内存将整个数据库存⼊内存中。我们**需要最⼩化每次从磁盘读取内容或运行查询时所带来的影响**

我们主要**讨论磁盘存储**的内容，分割线以上的内容将在15-721中学习，在⾼级课程中，我们假定数据库始终是运⾏在DRAM中的 。在这个课程中，我们不会去讨论关于将数据存⼊CPU缓存相关的东⻄，因为⽬前我们并不介意将数据存⼊速度缓慢的磁盘中

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20220122211130771.png" alt="image-20220122211130771" style="zoom: 50%;" />

现在出现了非易失性的内存，也就是说速度比SSD快，而且断电后不会丢失。

设计DBMS的目标：

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20220122212517310.png" alt="image-20220122212517310" style="zoom:67%;" />

对磁盘顺序访问比随机访问要快得多，所以**DBMS的宗旨就是尽量将用户的随机访问转换成对磁盘的顺序访问**

## 基于磁盘的DBMS

数据库全部在磁盘上，数据库文件中的数据被组织成页，第一页是目录页。为了对数据进行操作，DBMS需要将数据带入内存中。buffer pool负责管理数据从内存到磁盘中的来回移动。

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20211015193526614.png" alt="image-20211015193526614" style="zoom: 50%;" />

DBMS还有一个可以执行查询的执行引擎。执行引擎将向缓冲池请求特定的页，buffer pool将负责将该页面放入内存，并给执行引擎一个指向内存中该页的指针。buffer pool manager将确保当执行引擎在这部分内存上运行时，页面就在那里

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20211015193616580.png" alt="image-20211015193616580" style="zoom: 50%;" />

## **DBMS vs. OS**

DBMS的一个高级设计目标是**支持超过可用内存量的数据库**。我们希望DBMS能够在等待从磁盘获取数据时处理其他查询。这和OS很像

不使用OS的原因是：DBMS（几乎）总是想要自己控制事情，并且可以在这方面做得更好，因为**它更了解正在访问的数据和正在处理的查询**，它可以做出最佳选择（例如：预取，更好的替换策略，更好的调度等）。但是OS什么都不知道，它只能看到看到一些读写系统调用。操作系统就像是⼀辆通⽤货⻋，然而我们可以像保时捷或法拉利那样调整我们的系统，使其完全符合我们的应用需求。

另一个原因是：OS并不是我们的朋友，它可能会做出某些对我们的数据库系统有害的决策。



此处我们需要关注两个问题：

- 我们如何表示磁盘上文件中的数据
- 我们实际该如何管理内存以及在硬盘间来回移动数据 

## File Storage

在其最基本的形式中，**由操作系统来管理磁盘，DBMS将数据库作为文件存储在操作系统上，操作系统将文件存储在磁盘上**。有些可能使用文件层次结构，另一些可能使用单个文件(例如，`SQLite`)。

操作系统对这些文件的内容一无所知（也不需要关心），只有DBMS才知道如何解读它们的内容，因为它是以一种特定于DBMS的方式进行编码的。比如说用记事本打开DBMS在OS上的文件可能出现的是乱码，而如果用DBMS打开，就会看到文件被切割成了一个个的页。

DBMS的存储管理器（storage manager，有时也被称为存储引擎，是我们的数据库系统中的一个组件）负责管理磁盘上的数据库的文件。它将这些文件表示为页的集合。它还跟踪已读取和写入页面的数据，以及这些页面中有多少空闲空间

## Database Pages

DBMS 将数据库跨一个或多个文件组织在称为页的固定大小的数据块中。页面可以包含不同类型的数据（tuple、index等）。大多数系统不会在页面中混合这些类型。有些系统将要求页面是*self-contained*（自解释的）的，即一个页相关的所有metadata在同一页中。

**每个页面都有一个唯一的标识符**。如果数据库是单个文件，则页面 ID 可以只是文件偏移量。大多数 DBMS 都有一个indirection 层，将页面 ID 映射到文件路径和偏移量。系统的上层会询问特定的页码。然后，存储管理器必须将该页码转换为文件和偏移量才能找到该页。

**大多数DBMS使用固定大小的页**，以避免支持可变大小的页所需的工程开销。例如，对于可变大小的页，删除一个页可能会在文件中创建一个DBMS无法轻易地用新页面填充的空间。

DBMS的页：

- 硬件页（4KB）
- OS页（4KB）
- 数据库页（1~16KB）

**存储设备只能保证对硬件页面大小的页的原子操作**。如果硬件页面为4KB，并且系统试图将4KB写入磁盘，要么所有4KB都将写入，要么都不写入。这意味着，如果我们的数据库页大于我们的硬件页，DBMS将不得不采取额外的措施，以确保数据安全写入

## page存储架构

文件主要有两种类型：heap file和sorted file，数据库会根据关系的访问模式的I/O成本来选择要使用的文件类型。I/O相当于从磁盘读取1页或向磁盘写入1页，并根据其访问模式中的插入、删除和扫描操作对每个文件类型进行I/O计算。

### Heap File Organization 

堆文件是页的无序集合，记录在页中以随机的顺序存储。即，一条记录可以放在文件中的任何地方，只要那里有足够的空间存放这条记录，记录间不用考虑先后顺序的。 如果我⼀个接⼀个地插⼊tuple，我并不能保证它们是按照我插⼊的顺序保存在磁盘上的。通常每个关系使用一个单独的堆文件。

由于这种组织方式并不关心记录间的顺序，因此DBMS只需要登记堆文件中哪些页面中是存储了数据的（数据页），哪些页面是空闲的（空闲页）。具体可以采用以下两种表示形式：

- 在linked list实现中，每个数据页都包含records、空闲空间跟踪器（**free space tracker**）以及指向下一页和上一页的指针（字节偏移量）。有一个header page作为文件的开头，包含两个指针，一个指向空闲页列表，一个指向数据页列表。**如果 DBMS 正在寻找特定页面，它必须对数据页面列表进行顺序扫描，一个一个比较page id，直到找到它正在寻找的页面，I/O开销较大。**

  当需要空间时，分配一个空page，并append到空闲page部分链表的前面。当空闲page被装满了，就将它从空闲部分移动到Full pages部分（通常是将它移动到Full pages链表的前面，因为这样就不需要遍历整个链表来获取最后一个节点的指针；也可以维护一个指向最后一个节点的指针）

  <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230214164634261.png" alt="image-20230214164634261" style="zoom:50%;" />

- **Page Directory**实现，在header page中，**维护了page id和它们所处位置的映射关系**，相当于一个目录，用于跟踪数据页面的位置以及每个页面上的可用空间量

  <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20211016132127968.png" alt="image-20211016132127968" style="zoom:50%;" />

  **DBMS必须保持目录页中的引用与data page同步**，但实际上很难做到同时写两边的page数据。比如，我修改了某一个data page，但是还没来得及同步目录页中的引用，系统就崩溃了。当我再次上线时，只能扫描所有的data page。（当系统恢复后，我们可以通过某种⽅式来定位崩溃发⽣的原因。比如checksum，在目录页中对每个data page都放一个checksum，当从故障恢复后，我去查看最后⼀个page中所计算出的checksum，发现它的值和预定的值并不一致，因为数据并没有被写进去，因此，我就会得到⼀个报错 ）

  

## page 布局

每个page中都有⼀个header ，其中包括：

- Page size.
- Checksum.
- DBMS version.
- Transaction visibility.
- Self-containment. (Some systems like Oracle require this.)

一种布局数据的方法(strawman approach)是在每个页的header中跟踪当前页面中存储了多少元组，如果我们的tuple的长度是固定的，就可以在每次添加新元组时根据header中的偏移量追加到末尾。但是，当元组被删除或元组具有可变长度的属性时，就会出现问题。

- 如果tuple是可变长度的，那么你想插⼊的那个位置可能就没有⾜够的空间去保存那个tuple；
- 如果tuple被删除，我需要去维护header中的`metadata`，记录这个位置可以写入数据，或者对这个page进行遍历，然后找到插入tuple的位置。

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20211016230501564.png" alt="image-20211016230501564" style="zoom:67%;" />

在一个page中，有两种不同的方式来布局数据：

- slotted-pages，类似bitmap

  - Header 跟踪使用的槽数、最后使用的槽的起始位置的偏移量，以及跟踪每个元组开始位置的槽数组。 
  - 槽数组将槽映射到tuple的起始位置的偏移量
  - 添加一个元组，槽数组会从头到尾增长，元组会从尾到头增长。当槽数组和元组数据相遇时认为页面已满

  <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20211017094306630.png" alt="image-20211017094306630" style="zoom:50%;" />

  <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20211017094116692.png" alt="image-20211017094116692" style="zoom:50%;" />

  没有人会在⼀个page内保存不同表的tuple ，因为你就不得不去维护元数据，⽐如你就得说明，这个tuple来⾃表1，那个tuple来⾃表2。⼀般来讲，我们想将⼀整个tuple放在⼀个单个page中，因为当我们想去访问这个tuple的时候，它就在这个page上，那我们直接读取就⾏，⽽不是分散在多个page上，还需要通过⼀些额外的元数据和指针来表示我们所要查找的剩余部分的数据的page所在位置。 

  

  <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20211017095701929.png" alt="image-20211017095701929" style="zoom:50%;" />

  删除tuple 2后，并没有将整个页面重新组织，

  <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20211017095730098.png" alt="image-20211017095730098" style="zoom:67%;" />

  **每个tuple有一个独特的record id来寻址，这个record id由页号加上此record在该页中的偏移量或者slot号组成**

  

- log-structured：DBMS 只存储log记录，而不是存储元组。 

  - 将有关数据库修改方式（插入、更新、删除）的记录存储到文件中。 

    <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20211017111102305.png" alt="image-20211017111102305" style="zoom: 50%;" />

  - 为了读取记录，DBMS 向后扫描日志并“重新创建”元组以找到它需要的内容。

  - 快速写入，读取速度可能较慢。 

  - 适用于仅追加存储（append-only storage），因为 DBMS 无法返回并更新数据。 

  - 为避免长时间读取，DBMS 可以具有索引以允许它跳转到日志中的特定位置。它还可以定期压缩日志。 （如果它有一个元组，然后对其进行更新，它可以将其压缩为仅插入更新的元组。）压缩的问题在于 DBMS 最终会进行写入放大。 （它一遍又一遍地重写相同的数据。）
  
  log-structured常用于kv数据库，对于kv数据库来说，每次修改就相当于修改了一个record的全部，想要读取某个record只需要读取最新的log即可；而对于关系数据库来说，读取最新的log只能知道一个record中的一条字段，还需要往前读取其他的log。
  
  周期性压缩日志：相当于raft中对log建立一个快照
  
  <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20220123232316445.png" alt="image-20220123232316445" style="zoom:50%;" />
  
  id=4的record不在当前的page中insert，所以对它的delete的log无法在当前page中压缩。所以我们可以将多个page压缩成一起。
  
  ![image-20220123232555008](https://raw.githubusercontent.com/BoL0150/image2/master/image-20220123232555008.png)

## Tuple Layout

元组本质上是一个字节序列。 DBMS 的工作是将这些字节解释为属性类型和值。

- 元组header：包含有关元组的元数据。 
  - DBMS 并发控制协议的可见性信息（即关于哪个事务创建/修改了该元组的信息）。 
  - NULL 值的bit map。 
  - 请注意，DBMS 不需要在此处存储有关数据库schema的元数据。
- 元组数据：属性的实际数据。 
  - 属性通常按照我们在创建表时指定的顺序存储。 
  - 大多数 DBMS 不允许元组超过page的大小。
- 唯一标识符： 
  - 数据库中的每个元组都分配有一个唯一标识符。 
  - 最常见：page id +（offset或槽位（slot））。 
  - 应用程序不能依赖这些 id 来表示任何意思。

Denormalized Tuple Data：如果两个表相关，DBMS 可以“pre-join”它们，因此这些表最终位于同一页面上。这使得读取速度更快，因为 DBMS 只需加载一页而不是两个单独的页面。但是，它使更新成本更高，因为 DBMS 需要为每个元组提供更多空间。 

# Lecture 4 Database Storage Part II

## Data Representation

数据表示是 DBMS 如何存储一个值的字节。有五种高级数据类型可以存储在元组中：integers, variable-precision numbers, fixed-point precision numbers, variable length values, and dates/times

### Integer

大多数 DBMS 使用 IEEE-754 标准指定的C/C++ 类型存储整数。这些值是固定长度的。示例：`INTEGER`、`BIGINT`、`SMALLINT`、`TINYINT`

### Variable Precision Numbers

这些是使用 IEEE-754 标准指定的C/C++ 类型的**不精确、精度可变**的数字类型。这些值也是固定长度的。**对可变精度数的运算比任意精度数的计算速度更快**，因为 CPU 拥有能够高效执行这些操作的指令。但是，由于某些数字无法精确表示，因此**在执行计算时可能会出现舍入错误**。示例：`FLOAT`，`REAL`

### Fixed-Point Precision Numbers

这些是具有任意精度和比例的数字数据类型。它们通常以**精确的、可变长度**的二进制表示形式（几乎像一个字符串）存储，并带有额外的元数据，这些元数据会告诉系统诸如数据长度和小数点应该在哪里。**当舍入错误不可接受时使用这些数据类型，但 DBMS 会付出性能代价来获得这种准确性**。示例：`NUMERIC`，`DECIMAL`。

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20220123232812201.png" alt="image-20220123232812201" style="zoom:50%;" />

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20220123232905757.png" alt="image-20220123232905757" style="zoom: 50%;" />

### Variable-Length Data

这些代表**任意长度**的数据类型。它们通常与一个header一起存储，该header跟踪字符串的长度，以便轻松跳转到下一个值。它还可能包含数据的checksum。

大多数 DBMS 不允许元组超过单个页面的大小。**为了在tuple中存储大于页面的值，DBMS 使用单独的溢出存储页面（overflow storage page）并且该tuple中包含一个指向溢出页的引用**。这些溢出页可以包含指向其他溢出页的指针，直到可以存储所有数据。

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20211017131954104.png" alt="image-20211017131954104" style="zoom:67%;" />

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20220123233429070.png" alt="image-20220123233429070" style="zoom:50%;" />

**某些系统允许我们将这些大的值（BLOB，Binary Large Object）存储在外部文件中，然后元组不包含该属性的数据，而是包含指向该文件的指针或者一个文件路径**。例如，如果数据库正在存储照片信息，DBMS 可以将照片存储在外部文件中，而不是让它们占用 DBMS 中的大量空间。这样做的一个缺点是 DBMS 无法操作该文件的内容。因此，没有持久性或事务保护。

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20211017132804582.png" alt="image-20211017132804582" style="zoom:50%;" />

示例：`VARCHAR`、`VARBINARY`、`TEXT`、`BLOB`

### Dates and Times

日期/时间的表示因不同的系统而异。通常，这些表示为自 unix epoch以来的某个单位时间（微/毫秒）秒。

示例：`TIME`，`DATE`，`TIMESTAMP`

### System Catalogs

为了让 DBMS 能够解码元组的内容，它**有关数据库的元数据存在内部的catalog中**。元数据将包含有关数据库具有哪些表和列（tables、columns、indexes、views）以及它们的类型和值的顺序的信息（users、permissions）。

大多数 DBMS 将它们的数据库的catalog存储在它们自己内部。他们使用特殊代码来“引导”这些目录表，可以通过某种底层方法来访问catalog。

您可以查询 DBMS 的内部 INFORMATION_SCHEMA catalog以获取有关数据库的信息，每个DBMS 都具有非标准的快捷方式以获取catalog。

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20211017140326051.png" alt="image-20211017140326051" style="zoom:50%;" />

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20211017140403045.png" alt="image-20211017140403045" style="zoom:50%;" />

数据库将自己的metadata也作为系统表存起来

## Workloads

数据库系统有许多不同的工作负载。工作负载，我们指的是系统必须处理的请求的一般性质。本课程将重点关注两种类型：Online Transaction Processing 和 Online Analytical Processing.

`OLTP`：在线事务处理 

- `OLTP` 工作负载的特点是**运行速度快、运行时间短、一次对单个实体进行操作的简单查询以及重复性操作**。 `OLTP` 工作负载通常处理的写操作多于读操作。 `OLTP` 工作负载的一个示例是 Amazon 店面。用户可以将东西添加到他们的购物车，他们可以进行购买，但这些操作只会影响他们的帐户。

  这通常是人们最先构建的那种应用程序。

`OLAP`：在线**分析**处理 

- `OLAP` 工作负载的特点是**长时间运行、复杂的查询，读取数据库的大部分内容**。**在 `OLAP` 工作负载中，数据库系统分析并从 `OLTP` 端收集的现有数据中导出新数据**。 `OLAP` 工作负载的一个例子是亚马逊计算这些地理位置在一个月内购买最多的五个项目。（类似数据科学，或者说大数据。比如Hadoop就是OLAP）

`HTAP`: Hybrid Transaction + Analytical Processing 

- 最近流行的一种新型工作负载是`HTAP`，它就像是试图在同一个数据库上同时做`OLTP`和`OLAP`的组合（`NoSql`）

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20211017204830251.png" alt="image-20211017204830251" style="zoom:50%;" />

## Data storage models

关系模型并没有指定应该怎么存储tuple，DBMS 可以以更适合 `OLTP` 或 `OLAP` 工作负载的不同方式存储元组

行存储：将一行连续地存放；列存储：将一列连续地存储

本学期到目前为止，我们一直在假设使用 n-ary 存储模型（storage model，`NSM`）（又名“行存储”）

- 在 n 元存储模型中，DBMS **将单个元组的所有属性连续存储在单个页面中**，因此 `NSM` 也称为“行存储”。这种方法非常适用于 `OLTP` 工作负载，其中请求大量都是insert，并且事务往往只操作单个实体。这是理想的，因为只需要一次fetch就能够获取单个元组的所有属性，我们要去访问的数据量在粒度上足够小（~~一次取一行数据~~），这样就能访问单个实体

  <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20211017211312184.png" alt="image-20211017211312184" style="zoom: 50%;" />

  <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20211017211404582.png" alt="image-20211017211404582" style="zoom:50%;" />

  - 优点： 快速插入、更新和删除。适用于需要整个元组的查询。

  - 缺点： 不适合扫描表的大部分或属性的子集。这是因为它会获取处理查询不需要的数据来污染缓冲池

    比如，我需要查询一个表中的所有的tuple的hostname，但是我们没有办法直接从磁盘中获取这些数据，必须拿到整个page才行。所以为了执行这条查询，我们在内存中有了这些我们根本不会使用的列。

    <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20211017212144370.png" alt="image-20211017212144370" style="zoom: 50%;" />

- 分解存储模型 (Decomposition Storage Model，DSM) 

  在分解存储模型中，**DBMS 在一个数据块（page）中连续存储所有元组的单个属性（列）**。因此，它也被称为“**列存储**”。此模型非常适合具有许多只读查询的 `OLAP` 工作负载，这些查询对表的属性的子集执行大量的扫描。将同一个表中的tuple的不同属性拆分，同一个属性的值放在一个page中。

  <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20211017213758226.png" alt="image-20211017213758226" style="zoom:50%;" />

  优点： 

  - 减少查询执行期间浪费的工作量，因为 DBMS 仅读取该查询所需的数据。 
  - 更好的压缩，因为同一属性的所有值都是连续存储的。（如果在属性中有重复的值存在，就可以将其压缩）

  缺点： 由于一个元组被拆分，有可能在多个不同的page中，点查询、插入、更新和删除速度较慢



要在使用列存储时将元组重新组合在一起，有两种常用方法： 

- 最常用的方法是固定长度的偏移量。假设属性都是固定长度的，如果我想要找列1中第二个值对应的tuple中在其他列的属性值，DBMS 可以计算其他属性在它们的page中的偏移量（2*属性长度）。

  为了适应可变长度字段，系统可以填充字段以使它们的长度都相同，或者使用采用固定大小整数并将整数映射到值的字典

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20211017215940656.png" alt="image-20211017215940656" style="zoom:50%;" />

通常的配置是这样的：会有前端`OLTP`数据库（mysql、MongoDB等），以及后端⼤型数据仓库（Hadoop，Spark等），对其中⼀个`OLTP`数据库实例进⾏⼀系列更新操作 ，然后可以进⾏某种被称为ETL的操作（ETL是将业务系统的数据经过抽取、清洗转换之后加载到数据仓库的过程），并在数据仓库进行所有的分析（当你看到⼈们买了这个东⻄，⼜买了那个东⻄ ，这是在`OLAP`处进⾏的）。接着，将分析后的新信息推到前端`OLTP`数据库，通过`OLTP` 应⽤程序进⾏对外暴露。

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20211017222400198.png" alt="image-20211017222400198" style="zoom:50%;" />

# Lecture 5 Buffer Pools

DBMS 负责管理其内存并从磁盘来回移动数据。由于大多数情况下**无法直接在磁盘中操作数据**，因此任何数据库都必须能够有效地将磁盘上表示为文件的数据移动到内存中，以便可以使用（这是冯诺依曼架构的特点）。这种交互的图表如图 1 所示。 DBMS 面临的一个障碍是最小化移动数据的延迟问题。理想情况下，它**应该表现就好像数据已经全部在内存中一样**

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20211018084829165.png" alt="image-20211018084829165" style="zoom:50%;" />

考虑这个问题的另一种方式是空间和时间控制。

- 空间控制是指页面写入磁盘的物理位置。空间控制的目标是将经常一起使用的页面在磁盘上尽可能地保持在物理上靠近。
- 时间控制是指何时将页面读入内存以及何时将它们写入磁盘。时间控制旨在最大限度地减少用户等待从磁盘读取数据的次数

所以，今天我们要讨论的内容是

- 如何去构建⼀个Buffer池管理器（Buffer Pool Manager），某些系统会将Buffer池管理器叫做buffer缓存，它是由数据库系统管理的内存。
- 当需要释放内存空间时，我们该如何使⽤不同的策略来决定让哪些pages写出到磁盘上，可以通过哪些额外的优化来最⼩化这种影响

比如当我们对一个文件进行修改，我们需要先把这个文件从磁盘中读取到内存中，再对内存中的文件进行修改。在我们按下Ctrl + s后，这个文件的修改才被写回磁盘中。

## Buffer Pool Organization 

本质上来讲，Buffer Pool需要我们在数据库系统内部分配⼀块很⼤的内存区域，我们会去**调⽤`malloc` ，拿到⼀些内存块，并将我们从磁盘中读取到的所有page放⼊⾥⾯**，是这些page的内存缓存。**这段内存完全是由数据库系统来控制的，⽽不是操作系统来分配这些内存的** 

缓冲池的内存区域组织为固定大小的page数组。每个数组条目称为一个frame。当 DBMS 请求一个页面时，一个精确的副本被放入缓冲池的一个frame中。然后，当请求页面时，数据库系统可以首先搜索缓冲池。如果未找到该页面，则系统从磁盘中获取该页面的副本。

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20211018091408396.png" alt="image-20211018091408396" style="zoom:50%;" />

### Buffer Pool Meta-data

首先，page table是一个内存中的哈希表，用于跟踪当前内存中的页。它将**页ID 映射到缓冲池中的frame位置**。由于缓冲池中页的顺序不一定反映磁盘上的顺序，这个额外的indirection layer允许识别缓冲池中的页面位置。请注意，不要将page table与page directory混淆，page directory是从页 ID 到数据库文件中页位置的映射，存在于磁盘中，必须持久化。

page table还维护每页的额外元数据：dirty-flag和 pin/reference counter。

- 每当修改页面时，线程都会设置该页面的dirty-flag。这向存储管理器表明页必须写回磁盘。
- pin/reference Counter 跟踪当前正在访问该页面（读取或修改它）的线程数。线程必须在访问页面之前增加计数器。如果页面的计数大于零，则不允许存储管理器从内存中驱逐（evict）该页面

### Locks vs. Latches

Locks：Lock是一种更**高级别**的逻辑原语，用于保护数据库的内容（例如，元组、表、数据库）免受其他事务的影响。**事务将在其整个持续时间内持有锁**。数据库系统可以向用户公开在运行查询时持有哪些锁。锁需要能够回滚更改。

Latches：Latch 是 DBMS 用于其内部数据结构（例如，哈希表、内存区域）中的关键部分的**低级**保护原语。**Latch 仅在正在进行的操作期间保持**。Latch不需要能够回滚更改

### Memory Allocation Policies

根据两种策略为缓冲池分配数据库中的内存。

- 全局策略（Global Policies）处理 DBMS 应该做出的决策，以**使正在执行的整个工作负载受益**。它考虑所有活动事务以找到分配内存的最佳决策。
- 本地策略（Local Policies）做出决策，**使单个查询或事务运行得更快，即使它对整个工作负载不利**。本地策略将frame分配给特定事务，而不考虑并发事务的行为。

大多数系统使用全局和局部策略的组合。在实现第⼀个Project时使⽤全局策略⽐较好，因为它只需要找到最近最少使⽤的page，将它移除即可，即便它对于某个特定查询来说会变得很糟糕。

## Buffer Pool Optimizations

有多种方法可以优化缓冲池以使其适应应用程序的工作负载

- Multiple Buffer Pools

  DBMS 可以为不同的目的维护多个缓冲池（i.e per-database buffer pool, per-page type buffer
  pool）。我们可以分配多块内存区域 ，每个区域都有它们⾃⼰的page表 ，每⼀个都有⾃⼰的⼀套page id和frame的映射关系。然后，**每个缓冲池都可以采用为其内部存储的数据量身定制的本地策略**。这种方法可以帮助减少试图访问Buffer池的不同线程间争抢Latch的情况发⽣并提高局部性。

  将所需页面映射到缓冲池的两种方法是object ID 和散列

  - Object ID 涉及扩展record ID 以包含有关每个缓冲池正在管理哪些数据库对象的元数据。然后通过对象标识符（object identifier），可以维护从对象到特定缓冲池的映射

    <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20211018180901418.png" alt="image-20211018180901418" style="zoom:50%;" />

- Pre-fetching

  DBMS 还可以通过基于查询计划预取页面进行优化。然后，**在处理第一组页面时，可以将第二组页面预取到缓冲池中**。 DBMS 通常在顺序访问多个页面时使用此方法

  <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20211018190018151.png" alt="image-20211018190018151" style="zoom: 50%;" />

- Scan Sharing（扫描共享）

  ![image-20211018201921534](https://raw.githubusercontent.com/BoL0150/image2/master/image-20211018201921534.png)

  <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20211018202005295.png" alt="image-20211018202005295" style="zoom:50%;" />

  <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20211018202028931.png" alt="image-20211018202028931" style="zoom:50%;" />

  Q1的查询结束后，Q2接着之前的查询

  <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20211018202112206.png" alt="image-20211018202112206" style="zoom:50%;" />

  <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20211018202258582.png" alt="image-20211018202258582" style="zoom:50%;" />

  

- Buffer Pool Bypass

  **顺序扫描运算符不会将获取的页面存储在缓冲池中以避免开销**。相反，内存对于正在运行的查询是**本地的**（local）。DBMS 会分配⼀⼩块内存给执⾏查询的那条线程，当该线程~~从磁盘中读取page~~查询page时，如果该page不在buffer池中，那么它必须从磁盘中拿到该page，这⾥并不会将它放⼊buffer池中，⽽是将它放⼊本地内存。当查询完成时，所有这些page就会被丢弃，这么做的原因是避免去page表中进⾏查询所带来的开销 （Page 表是一个hashtable，对应的条⽬是带锁的（latch），使用它需要付出一定代价）

  这种方法只有当线程操作的是中间结果和扫描的量不⼤的时候才⾏（可以放入bypass中），如果在执⾏排序，它可能需要TB级⼤⼩的内存，此时就需要使用buffer池，因为这样我们才可以根据需要将⼀些pages放回到disk中以腾出空间，然后循环往复，直到排序完成。

  如果操作员需要读取在磁盘上连续的大量页面序列，这很有效。 Buffer Pool Bypass 也可以用于临时数据（排序、join）。

  

## OS Page Cache（重要）

**DBMS属于用户程序，建立在OS上**，在正常情况下，DBMS的缓冲池要从OS的文件系统中获取page，所以DBMS的一个数据有两份缓存，一份缓存在DBMS的缓冲池中，一份缓存在OS文件系统的page cache中。比如read系统调用要先去内核文件系统中的page cache中查找，如果不在page cache中，就先从磁盘中读取到物理内存，映射到内核的page cache中，再将数据读取到DBMS用户空间的缓冲池中。write系统调用也要先写入到内核中的page cache，也就是说此时并未落盘。

但是这样就会造成page的冗余副本，所以**大多数 DBMS 使用直接 I/O （`O_DIRECT`）绕过操作系统的缓存**。DBMS的缓冲池从磁盘中获取数据的时候是不需要经过文件系统的page cache的，直接将数据拉到物理内存，然后映射到DBMS用户空间中指定的虚拟地址。相应的，write调用也会直接绕过page cache，所以理论上，write调用结束后数据应该已经持久化了，但是除了文件数据本身，文件还有一些重要的元数据，比如文件的大小，也会影响数据的完整性，而direct I/O只是对文件数据本身有效，文件的元数据读写还是会经过内核page cache，所以使用了direct I/O读写文件，依然需要使用fsync来刷新文件的元数据。

除此之外，还有一个问题，操作系统对物理内存的替换策略和DBMS对自己用户空间中缓冲池（虚拟地址）的替换策略是不一样的：

- 操作系统管理的是物理内存到页表的映射，并且负责管理**物理内存**的替换（swap到磁盘中）；而DBMS是用户程序，DBMS中的缓冲池负责管理的是自己的用户空间中的**虚拟页**。

比如DBMS正在修改自己用户空间的缓冲池中的一个虚拟页，该页被pin住了，是不会被缓冲池替换出去的，但是从OS的层面来说，如果OS发现物理空间消耗完了，就有可能将这个虚拟页对应的物理页替换到磁盘上，这个替换对用户进程来说是不可见的，用户看的是自己的页表，当DBMS再次访问这个虚拟页的时候，会发生一个page fault，进入内核，将此页从磁盘再替换回来。

在生产环境中一般会把OS的swap禁用，然后再使用direct IO，数据库基本就可以完全按照自己的意愿来管理内存了，就不会出现OS的物理页替换与DBMS的缓冲池替换策略冲突的情况。（swap本来是为了处理以前机器内存小而产生的，现在的服务器的内存充足，开了swap反而可能会影响性能）

## Buffer Replacement Policies

当 DBMS 需要释放一个frame来为新页面腾出空间时，它必须决定从缓冲池中驱逐（evict）哪个页面。

替换策略是 DBMS 实现的一种算法，它决定在空间不足时从缓冲池中驱逐哪些页面。

替换策略的实施目标是提高正确性、准确性、速度和减少元数据开销

### Least Recently Used (LRU) 

最近最少使用替换策略维护每个页面上次访问时间的时间戳。可以将此时间戳存储在单独的数据结构中，例如队列，以便进行排序并提高效率。 **DBMS 选择驱逐具有最旧时间戳的页面**。此外，页面按排序顺序保存，以减少逐出时的排序时间

### CLOCK

CLOCK 策略是 LRU 的变体，不需要每个page单独的时间戳。在 CLOCK 策略中，每个page都有一个引用位。当一个页面被访问时，设置为 1。

为了可视化这一点，将页面组织在一个带有“时钟指针”的循环缓冲区中。当空间不足时，扫描检查页面的位是否设置为 1。如果是，则设置为零，如果不是，则驱逐它。通过这种方式，时钟指针会记住驱逐之间的位置（下一次需要替换时，从上一次驱逐的位置继续向后扫描）。

**此方法是近似的LRU**。

### Alternatives

LRU 和 CLOCK 替换策略存在许多问题。

LRU 和 CLOCK 容易受到sequential flooding的影响，其中缓冲池的内容由于顺序扫描而被破坏。由于顺序扫描读取每一页，读取的页面的时间戳可能无法反映我们真正想要的页面。换句话说，**最近使用的页面实际上是最不需要的页面**。（详见slides）

有三种解决方案可以解决 LRU 和 CLOCK 策略的缺点。

- 一种解决方案是 LRU-K，它跟踪最后 K 个引用的历史作为时间戳，并计算后续访问之间的间隔，可以看到**哪⼀个page的上⼀次和下⼀次访问的间隔时间最⻓ ，哪个page就是最近最少被使⽤的**。此历史记录用于预测下一次访问页面的时间。
- 使⽤多个buffer池，让每个查询本地化。当想去判断该从我的查询中移除哪个page时 ，会去移除对当前查询⽽⾔最近最少使⽤的那个page，⽽不是从全局的⻆度来看
- 最后，priority hint 允许事务在查询执行期间根据每个页面的上下文告诉缓冲池页面是否重要

### Dirty Pages

![image-20211019135400129](https://raw.githubusercontent.com/BoL0150/image2/master/image-20211019135400129.png)

内存中的脏页暂时不写回磁盘，等待某一个时机大量的在磁盘中相邻的脏页集体写回磁盘或者这个脏页在内存中被驱逐时写回磁盘。问题：如果脏页在被写回磁盘之前，机器断电了怎么办？

WAL：修改了页之后可以先不写回磁盘，但是要将这次修改作为log写回磁盘。日志和脏页至少要持久化其中一个，要不然机器断电后数据会丢失

# Lecture 6 Hash Tables

我们现在要讲的是如何支持DBMS的执行引擎（execution engine）从page读/写数据

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20211019142231349.png" alt="image-20211019142231349" style="zoom:50%;" />

在为 DBMS 实现数据结构时，有两个主要的设计决策需要考虑： 

- 数据组织：我们需要考虑如何布局内存以及在数据结构中存储哪些信息以**支持高效访问**。 
- **并发**：我们还需要考虑如何让多线程访问数据结构而不出现问题

## Hash Table

一个哈希表由两部分实现：

- hash function：告诉我们如何将一个大的空间映射到小的空间。将任意类型的key传入其中，返回一个32位或64位的整数（同一个key永远返回相同的数，不同的key有可能返回相同的数）。当前最先进的哈希函数是 Facebook 的 XXHash3

  <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20211019153355249.png" alt="image-20211019153355249" style="zoom:50%;" />

- Hashing Scheme：告诉我们在散列后如何解决哈希碰撞

所有的Hash函数都是不可倒推的，即可以通过Hash函数的输入计算出一个数，但是不可能通过这个结果（加上Hash函数）倒推出输入具体是什么数，因为Hash函数在计算的过程中会丢失信息。Hash函数分为加密和不加密的hash函数

- 对不加密的Hash函数，虽然我们无法推算出输入具体是哪一个，但是我们可以倒推出一些有可能的输入
- 对于加密的Hash函数，我们连一个有可能的输入都推算不出来
  - 比如sha256，此算法是完全公开的，但是完全无法推算出输入

## Static Hashing Schemes（静态hash）

静态散列方案是一种**哈希表大小固定**的方案。这意味着如果 DBMS 用完哈希表中的存储空间，那么它必须从头开始重建一个更大的哈希表，将所有的条目重新hash后再插入，这是非常昂贵的。通常，**新哈希表的大小是原始哈希表的两倍**。为了减少无用的比较次数，重要的是要避免哈希碰撞。通常，**我们使用两倍于预期元素数量的槽数**。

以下假设在现实中通常不成立： 

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20211019151236406.png" alt="image-20211019151236406" style="zoom:50%;" />



### 线性探测法

当碰撞发生时(当一个键的散列值已经被另个不同的键占用)，我们直接检查散列表中的下一个位置(将索引值加1)。这样的线性探测可能会产生三种结果:

- 该位置的键和被查找的键相同，命中
- **该位置的键和被查找的键不同**，继续查找，直接检查数组中的下一个位置(将索引值加1，到达数组结尾时折回数组的开头)
  - 查找到键为null ，散列表中不存在该键，结束查找操作。如果是插入操作的话，此时在该位置插入新的键值对。
  - 找到该键，命中。

**Hash查找时要使用双重保障，先根据Hash值寻址，找到了地址后还要再确认槽中的元素是否就是要找的元素**。

开放地址类的散列表的核心思想是与其将内存用作链表，不如将它们作为在散列表的空元素。这些空元素可以作为查找结束的标志。

删除一个条目更加复杂，因为如果只是简单地删除一个元素，会导致后面经过此处的查询被截断

所以有两种解决办法：

- 墓碑法

  在被删除的位置设置一个墓碑，后面经过此处的查询就可以越过此处继续向前搜索比较

  <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230214214818744.png" alt="image-20230214214818744" style="zoom:50%;" />

- 移动（movement）

  被删除键的右侧的所有键**重新插入散列表**。

  <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230214214923494.png" alt="image-20230214214923494" style="zoom:50%;" />



在算法课上讨论hashtable时，会假设所有的key都是唯一的，对于主索引（primary key）来说，这是可行的，但是在实际的数据集中，我们无法假设所有的key都是唯⼀的 。当出现重复的key时，有两种解决方法：

- 单独的链表：我们不是在hash表中存储值，而是存储一个指向单独存储区域的指针，该存储区域包含同一个键的所有值。 
- 冗余键：更常见的方法是简单地在表中多次存储相同的键。即使我们这样做，线性探测法仍然有效

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20211019172111658.png" alt="image-20211019172111658" style="zoom:50%;" />

### Robin Hood Hashing（劫富济贫）

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230214215810977.png" alt="image-20230214215810977" style="zoom: 33%;" />

## Dynamic Hashing Schemes（动态hash）

静态散列方案要求 DBMS 知道它想要存储的元素数量。否则，如果需要增大/缩小表，它必须重建表（将所有的entry重新散列后再插入）。

**动态散列方案能够按需调整散列表的大小，而无需重建整个表**。这些方案以不同的方式执行这种调整大小，可以最大化读取或写入

### Chained Hashing（拉链法）

这是最常见的动态散列方案。 DBMS 为散列表中的每个槽维护一个桶链表。**散列到同一个槽的键被简单地插入到该槽的链表中**

### Extendible Hashing

![image-20211019205925434](https://raw.githubusercontent.com/BoL0150/image2/master/image-20211019205925434.png)

### Linear Hashing

![image-20211019205913968](https://raw.githubusercontent.com/BoL0150/image2/master/image-20211019205913968.png)

# Lecture 7 Tree Indexes I

## Table Indexes

可以在数据库系统内部使用多种不同的数据结构。使用场景有内部元数据、核心数据存储、临时数据结构或表索引。

- 对于前三种场景来说，hashtable已经足够了。⽐如在内部系统中，我们⽆须经常在⾥⾯进⾏范围查询，在⼤多数情况下，我们所做的就是点查找，给定一个key，返回对应的value即可。

**表索引是表中一部分属性的一个副本，这些属性被组织或排序以使用这些属性的子集进行高效访问**。索引的本质就是一个小表，把大表中的一部分列抽出来进行排序。

因此，DBMS 可以查找表索引的辅助数据结构以更快地找到元组，而不是执行顺序扫描。

由于索引其实是该表的⼀个replica（副本），**DBMS 确保表（underlying table）和索引的内容在逻辑上始终同步**。如果我们更新了我们表中的⼀个tuple，我们也想让这个修改能够反应到我们的索引上。

在我们的系统中，**我们可以使⽤⼤量的索引来让查询变得更快，但我们还需要消耗⼀定的代价来对它们进⾏维护，这两者之间就存在了取舍问题**

- 需要耗费空间来维护索引
- 每次对大表进行修改也需要同步到索引

## B+Tree

B+Tree 是一种**自平衡树**数据结构，它保持数据有序并允许在 O(log(n)) 时间复杂度进行搜索、顺序访问、插入和删除。

几乎所有的现代 DBMS 都使用 B+Tree。有一种特定的数据结构称为 B 树，但人们也使用该术语来泛指一类数据结构。**原始 B-Tree 和 B+Tree 的主要区别在于 B-Trees 在所有节点中存储键和值，而 B+树仅在叶节点中存储值**。现代 B+Tree 实现结合了其他 B-Tree 变体的特性，例如 Blink-Tree 中使用的兄弟指针

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20211020110830297.png" alt="image-20211020110830297" style="zoom:67%;" />

B+树是⼀种多路查找树(M-way search tree) ，有如下属性：

- 完美平衡（每个叶结点都有相同的深度）
- 每个除了根节点之外的内部节点都最少是半满的（M/2 −1 <= num of keys <= M −1）
- 每个有k个键的内部节点有k+1个非空的子节点

B+Tree 中的**每个节点都包含一个键/值对数组**。这些对中的键源自索引的属性。内部节点和叶节点的值有所不同。

- 对于内部节点，值数组将包含指向其他节点的指针。

- **叶节点的值可以是record ID 或 元组数据**。

  - record ID 指的是指向元组位置的指针。
  - 具有元组数据的叶节点存储元组的实际内容。

  每个节点上的数组（几乎）按键排序

我们在数组的开头和末尾会有指向其他兄弟节点的指针，这⾥⾯存放的可能是node id 或者是page id，这允许我们跳到其他的节点上。

由于B+树天然是分块的，所以通常将B+树一个节点大小设置为一个页，方便从磁盘中读取

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20211020120447602.png" alt="image-20211020120447602" style="zoom:50%;" />

实际上，没有数据库会像这样去保存⼀个B+ Tree的叶⼦节点的key/value pair（键值对）数组，不会将key/value pair（键值对）和指针放在⼀起。⼀般来讲，它们是分别保存的，就像我们slotted page中的header⼀样，它可以告诉我们，我们page中所包含内容的相关元数据。

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20211020120531007.png" alt="image-20211020120531007" style="zoom:50%;" />

我们将key和value独⽴开来，当我们进⾏⼆分查找的时候 ，并不需要⽤到value，可以将所有的key直接放入缓存中。如果我们将它拆分开来 ，我们可以更⾼效地在key之间进⾏跳转。

这种方法的工作方式是，可以根据key数组中的offset找到value数组中对应的offset。

叶结点中的value有两种方法：

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20211020121614199.png" alt="image-20211020121614199" style="zoom:67%;" />

上面的数据库，在叶结点中的value是record id，而record的数据存放在另一个地方；

而下面的这些数据库，会默认对每张表的主键建立索引，这个索引的叶结点的value是实际的record数据。（**在 InnoDB 存储引擎中， 聚簇索引 就是数据的存储方式（所有的用户记录都存储在了叶子节点 ），也就是所谓的索引即数据，数据即索引**）也就是将整张表都存在索引中。如果需要对这张表的其他字段建立索引，则这些索引就是二级索引 （secondary index ），二级索引的叶结点中的value不再是record的数据，而是record的主键。需要拿着在二级索引中查询得到的主键，去聚簇索引中再查找一遍，得到实际的record。

## Clustered Indexes

**数据库中的table heap是⽆序的**（⼀张表的数据堆在⼀起，就是table heap），我们能以任何顺序将tuple插⼊到任何page中去 ，并不是按照插⼊的时间顺序进⾏排序 。

聚簇索引：**record的数据聚集在主键索引的叶子节点中** 。

- **在 InnoDB 存储引擎中， 聚簇索引 就是数据的存储方式（所有的用户记录都存储在了叶子节点 ），也就是所谓的索引即数据，数据即索引**

**当我们创建⼀张表的时候，我们可以定义一个聚簇索引 ，数据库系统会保证page中tuple的物理布局顺序与与索引一致**。

- 比如我要进行根据主键来进行范围查找的任务，因为tuple在物理page中保存的顺序与主键一致，我们只需要遍历某个叶⼦节点下的所包含的⼀⼩部分pages，就可以找到所有我想要的tuple。

  <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20220124232856812.png" alt="image-20220124232856812" style="zoom: 33%;" />
  
  而如果我所查找的key没有聚簇索引（比如对一个二级索引进行扫描，或者这个表的主键索引根本就不是聚簇的），**对非聚簇索引进行顺序扫描，那么我在B+tree中拿到的每个单个的record id都可能在不同的page中，需要进行大量的随机IO**
  
  <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20220124232940086.png" alt="image-20220124232940086" style="zoom:33%;" />
  
  解决方案是先将要访问的page id缓存起来，再将它们排序后，一次性地去磁盘上fetch

**在某些数据库（如MySQL）中总会使用聚簇索引**，如果你没有定义主键（primary key），那MySQL会帮你定义⼀个，它们会使⽤row id或者record id之类的东⻄作为主键，对于用户来说，它们是透明的，看不⻅它们。

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20220124233643869.png" alt="image-20220124233643869" style="zoom:50%;" />

如果是ap型的工作负载，那么节点的大小就应该大一些：因为经常需要扫描全表，节点越大，一次从磁盘fetch的数据就更多，磁盘IO就更少。

如果是tp型的，那么节点应该小一些：因为经常需要点查询，在索引中跳来跳去，一次从磁盘fetch的数据越少，点查询的速度就越快。

## 对非聚簇索引遍历的随机IO

对非聚簇索引的索引扫描明显会有大量的随机IO

- 如果tuple存在堆文件中：从非聚簇索引中拿到tuple id后还要去堆文件中拿真正的tuple，因为我们是对tuple中的某一列建的索引，而堆文件中的tuple是随机存放的，在索引上相邻的tuple在磁盘上可能相隔很远；
- 如果tuple存在另一个聚簇索引中：从非聚簇索引中拿到的是tuple的主键，再用主键去聚簇索引中回表，找到真正的tuple，此时也会有

## 可变长度的key

该如何处理可变⻓度的key ：

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20220124234448500.png" alt="image-20220124234448500" style="zoom:50%;" />

- 间接映射（indirection map） 

  <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20211028165309453.png" alt="image-20211028165309453" style="zoom:50%;" />

  将指向key的指针存放在key数组中，这里的指针实际上是key在这个node的page中所对应的offset值，而不是指向其他的page。这和slotted page中tuple的布局很像 

## 节点内部搜索

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20220124234820937.png" alt="image-20220124234820937" style="zoom:50%;" />

实际上很多数据库是使用线性搜索，因为线性搜索的时间相比于从磁盘fetch的时间可以忽略不计。

## 处理非唯一索引

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20211028170755200.png" alt="image-20211028170755200" style="zoom: 50%;" />

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20211028171024404.png" alt="image-20211028171024404" style="zoom:50%;" />

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20211028171056871.png" alt="image-20211028171056871" style="zoom: 50%;" />

节点内key的搜索方式：

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20211028171650810.png" alt="image-20211028171650810" style="zoom:50%;" />

## 优化

- Prefix Compression

  <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20211028173939752.png" alt="image-20211028173939752" style="zoom:50%;" />

- Suffix Truncation

  <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20211028174219856.png" alt="image-20211028174219856" style="zoom:50%;" />

- Bulk Insert：不是一个个地插入node，而是将大量现有的node建立一个B+Tree

  <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20211028174305707.png" alt="image-20211028174305707" style="zoom:50%;" />

- Pointer Swizzling

  <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20211028174430345.png" alt="image-20211028174430345" style="zoom:50%;" />



# Lecture 8 Tree Indexes II

 在B+Tree中，我们实际该如何维护这些重复的索引或重复的key

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20211029092855313.png" alt="image-20211029092855313" style="zoom:50%;" />

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20211029095414644.png" alt="image-20211029095414644" style="zoom:50%;" />

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20211029095515129.png" alt="image-20211029095515129" style="zoom:50%;" />



<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20211029130357027.png" alt="image-20211029130357027" style="zoom: 50%;" />

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20211029130453783.png" alt="image-20211029130453783" style="zoom:50%;" />

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20211029130525048.png" alt="image-20211029130525048" style="zoom:50%;" />

# Lecture 9 Index Concurrency Control

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20211029163617897.png" alt="image-20211029163617897" style="zoom:50%;" />

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20211029163756807.png" alt="image-20211029163756807" style="zoom:50%;" />

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20211020200401635.png" alt="image-20211020200401635" style="zoom:50%;" />

- Lock是一种宏观的锁，用于事务；
- 而latch是物理的微观锁，⽤来保护数据结构或对象的物理完整性，在操作过程中持有，在操作系统的世界中，被称为lock或者是mutex 。
  - **latch可以保护线程间共享的数据结构免受其他线程对该数据结构或对象同⼀时刻进⾏读写所带来的问题**，我们只会在对关键部分进⾏所需操作时持有这个 latch ，操作结束后会释放这个latch。我们所尝试要进⾏的操作，本质上来讲是原⼦性操作，我们不需要能够去回滚任何修改 。

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20220217181606362.png" alt="image-20220217181606362" style="zoom:50%;" />

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20211029173011946.png" alt="image-20211029173011946" style="zoom:50%;" />

latch分为两种模式：读模式和写模式

- 一个变量加了读锁可以和别的线程共享，**允许多条线程在同⼀时间去读取同⼀个对象**，只能读不能写。读锁也叫共享锁


- **加了写锁相当于该线程独占了这个变量**，别的线程不能对这个变量进行**任何**操作，不能读，不能写，更不能加锁。直到我完成操作前，没有⼈可以读取该对象 。写锁也叫独占锁

在共享对象的内部增加一个字段，这个字段就是一个锁对象。当一个线程读该对象的时候要加读锁，写的时候要加写锁。操作结束后把锁释放。

我们还可以改变锁的粒度，比如hashtable中，我们可以给整个hashtable对象维护一个锁，这就是最大的粒度，也就相当于整个hashtable不能并发。也可以给每个page对象维护一个锁，还可以给每个槽维护一个锁，这样整个hashtable中可能就有几万个锁，开销极大。

每次操作共享对象的时候都要获取该对象的锁，结束操作后要释放锁。

锁的实现方式：

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20220217221029511.png" alt="image-20220217221029511" style="zoom:50%;" />

这个实际上是睡眠锁，当一个线程获取锁失败后，就会进入内核态sleep，CPU进行上下文切换，调度另一个线程。缺点是上下文切换会浪费大量的资源

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20220217222852022.png" alt="image-20220217222852022" style="zoom:50%;" />

这个是自旋锁，比睡眠锁高效，但是会浪费CPU资源。

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20220217223551474.png" alt="image-20220217223551474" style="zoom:50%;" />

读写锁：在自旋锁的基础上实现。按照队列的方式先来后到获取锁，可以有多个线程获取读锁，但是**如果有线程在等待写锁，后面的读锁都要等待，不能直接读**。

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20220217223613275.png" alt="image-20220217223613275" style="zoom:50%;" />

CAS：可以利用CAS实现无锁（这不就是自旋锁的底层实现？按这个说法，用test_and_set也可以实现无锁？）

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20220217224540232.png" alt="image-20220217224540232" style="zoom:50%;" />

## Hash table并发控制

hash表比较好加锁，因为比如线性探测法，查询的方向永远是同一个方向，从上往下的，所以不会发生死锁。对于B+树，查询可能是不同的方向，**由于请求并保持的特性**（即请求下一个锁的时候还在持有上一个锁，只有获取了下一个锁后才能释放上一个锁，否则会出现空窗期），如果一个线程持有A锁请求B锁，另一个线程持有B锁请求A锁就会造成环路等待，从而出现死锁。

如果需要resize，可以对整个hash表加上大锁

如果是局部加锁：

- 可以将hash表分成段（或页），每一段有好几个槽。一个线程操作某一段时需要对这个段加锁，操作结束后将锁解开
- 也可以对每个槽加锁

## B+树并发控制

没有并发控制会怎么样：

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20220217225106225.png" alt="image-20220217225106225" style="zoom:50%;" />

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20220217225125228.png" alt="image-20220217225125228" style="zoom:50%;" />

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20220217225203406.png" alt="image-20220217225203406" style="zoom:50%;" />

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20220217225227594.png" alt="image-20220217225227594" style="zoom:50%;" />

当读进程进入叶结点后，此时没有获取叶结点的锁，如果另一个线程在此时删除了sibling的一个kv对导致sibling underflow，就会从该叶结点steal一个kv对。此时这个kv对明明还在树中，但是读线程却无法读到。

一个线程向B+树中删除或添加内容后导致了split或merge，破坏了共享的数据结构，导致后来的线程搜索不到想要的东西。

### Latch Crabbing

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20220217225746477.png" alt="image-20220217225746477" style="zoom:50%;" />

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20220217230249354.png" alt="image-20220217230249354" style="zoom:50%;" />

读的时候使用latch crabbing的作用是：防止出现空窗期，比如读线程在释放当前锁之前没有获取下一个锁，那么释放当前的读锁后，如果后面存在写进程就会趁虚而入，获取下一个的写锁，然后修改下一个节点，而读进程就需要等待，等写进程修改完后，读进程在下一个节点中读到的信息就不再是期望的信息

对B+树来说，我们无法判断当前节点是否会被修改，我们只有先查看子节点是否会导致split或merge，才能判断当前节点是否会被修改，所以在获取子节点的写锁的时候，我们必须要持有父节点的写锁：

- 在插入的情况下，如果子节点的容量**小于**MAX_SIZE，即使当前的节点被修改了，也**一定**不会导致split，也就**一定**不会导致父节点修改
- 在删除的情况下，如果子节点的容量**大于**MIN_SIZE，即使当前的节点被修改了，也**一定**不会导致merge，也就**一定**不会导致父节点修改

对以上两种情况，我们才可以释放父节点以及之前的所有的写锁，**不包括当前节点的写锁**，因为当前的节点会不会被修改还未知。

- 反之，父节点依然有可能会被修改（并不是一定会被修改），我们还是需要保持之前所有节点的写锁。

在B+ Tree中，当线程往下进⾏遍历时，**线程会通过⼀个stack来保存它⼀路上所持有的latch**

基本上在实现了上述的思想后，如果不引入ITERATOR的横向遍历，几乎可以不用改别的地方。比如**DELETE 的SIBLING PAGE是可以不用上锁保护起来，因为动到这个，PARENT一定是锁住的，不会有第2个线程可以走到这个SIBLING**。 前提是不利用叶子节点的横向指针，只能从ROOT走下来的情况。



用这种方法我们发现，在最悲观的情况下，我们担心最后会对根节点进行修改，每次对索引进行修改首先都要对根节点加写锁，在此期间其他线程完全无法使用索引，然而绝大多数情况下，对叶结点的修改是不会导致根节点的修改的，这就成了高并发情况下的瓶颈。

所以我们乐观地预计只有叶结点会修改，从根节点到叶结点一路上只加读锁，只有叶结点才加写锁。如果到了叶结点发现会导致merge或split，那么我们放弃这次修改，从根节点开始重新开始，这次采用全程加写锁的方式。

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20220217232404022.png" alt="image-20220217232404022" style="zoom:50%;" />

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20220217232518234.png" alt="image-20220217232518234" style="zoom:50%;" />

除此之外，对B+树叶结点的扫描可能会导致死锁，一个线程从左到右加写锁，一个线程从右到左加写锁，这种情况就会导致死锁（一个线程读，一个线程写也可能会导致死锁）。为了解决这种情况，我们需要规定B+树只能从一个方向扫描，按照规定的顺序获取写锁。

# 执行器

![image-20230215151735914](https://raw.githubusercontent.com/BoL0150/image2/master/image-20230215151735914.png)

输入一个sql语句，输出执行计划。算子按树形排列，数据从叶结点流向根节点，根节点的输出就是查询的结果

## 外部归并

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230215153307143.png" alt="image-20230215153307143" style="zoom:50%;" />

## aggregate

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230215155741921.png" alt="image-20230215155741921" style="zoom:50%;" />

# 连表算法

Join/连表的本质是把割开的关系重组起来，比如说xxx在一班，一班在3楼，把这两条关系组合起来，就可以得到“xxx在3楼”这个关系

连表时，有个原则，要尽量把小表（所占页数较少的表）放在左侧（此时这个小表也叫驱动表），后面会讲到，这会减少硬盘IO次数

join算子输出的内容有如下几种：

- 直接输出数据
  这种情况属于早物化，被join的都是完整的tuple，因此join操作结束后输出的就是完整的一行数据，join算子的结果输出给上层的算子（在此例中是project）后，上层的算子可以直接从join的完整数据中提取字段，无需再回到原始的表中找数据（这个操作简称回表）
- 输出record id
  这种属于晚物化，join后得到的一条数据里只含有在相对应的原始表中的record id，而不是全部的字段。上层的算子如果需要record中的其他字段，就要拿着record id回到原始表中查找

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230215181052112.png" alt="image-20230215181052112" style="zoom:50%;" />

假设R表有M个页大，含有m条数据，S表有N个页大，含有n条数据，我们首先通过分析硬盘I/O次数进而分析join操作的开销

join操作有时可以通过笛卡尔积来完成，先对两个表进行笛卡尔积，然后用谓词来筛选，但这非常低效，因为笛卡尔积导致的中间结果非常巨大，所以说除了笛卡尔积以外，DBMS的设计者更倾向于采用下面的几种join算法

## nested loop join

嵌套循环join，伪代码如下，外层循环是遍历R表的所有行，对于R表的每一行，再开一个内层循环，遍历S表的所有行，看r和s能不能连上

因为R表是小表，所以被放在外层循环中，也同时被放在了join算子的左侧，被称为outer table（对于基于硬盘的DBMS来说，所谓的“小表”一般是指所占的文件页少的，R表虽然行数多，但它比较窄，所占用的页数少，那么遍历R表所需的硬盘IO次数少）

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230215182007483.png" alt="image-20230215182007483" style="zoom:50%;" />

这种策略又名"stupid nested loop join"，因为它十分低效，当扫描S的整张表时，缓存池完全用不上：比如说S有3个页大，缓存池有1个页大，扫描S全表的时候会把缓存池灌满，然后不断地淘汰，然后当外层循环遍历到R表的下一条数据的时候，又要开始扫描S全表，但S表的第一个页早已被踢出缓存池

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230215182040421.png" alt="image-20230215182040421" style="zoom:50%;" />

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230215182159928.png" alt="image-20230215182159928" style="zoom:50%;" />

这种最原始的策略无法充分利用缓存池，基于这一点，我们可以对其做出优化

## block nested loop join

先把R的一个块读入内存，再把S的一个块读入内存，在这两个页内部相互匹配，这样R的一个块中的所有tuple就可以共享一块S的缓存

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230215183026358.png" alt="image-20230215183026358" style="zoom:50%;" />

![image-20230215183130348](https://raw.githubusercontent.com/BoL0150/image2/master/image-20230215183130348.png)

刚才的策略是在缓存池当中拿出一个页放R表，一个页放S表，一个页做join的输出缓存，如果可供join使用的缓存池很大，有B个页，我们拿出来1个页用作输出缓存，剩下的B-1个页里拿出B-2个页缓存outer table（也就是R表），1个页缓存inner table（也就是S表），之后执行如下描述的算法：

![image-20230215183718130](https://raw.githubusercontent.com/BoL0150/image2/master/image-20230215183718130.png)

![image-20230215184355837](https://raw.githubusercontent.com/BoL0150/image2/master/image-20230215184355837.png)

outer table的IO次数是不变的，一定是M次，因为要想join就必须要把整个外表读入内存。而且增加内表的缓存没有意义，因为不管内表的缓存有多大，内表的缓存总是会被刷新。

所以要想减少整体的IO次数就必须要减少内表的IO次数。由于对外表中的每一块都需要遍历整个内表，所以外表切分的块数决定了内表的IO次数。

- 当外表一次读入一个tuple时，内表IO次数为m*N
- 当外表一次读入一个page时，内表的IO次数为M*N
- 当一次性把整个外表都读入时，内表只用扫一遍就可以完成join，即IO次数为N次

无论怎么优化都会不止一次遍历右侧的表，这本质上是因为我们没有构建相应的索引，只能通过暴力地遍历去看有没有可以join的tuple。因此就有了如下的优化方式：**我们以inner table的参与join的那一列字段为key构建索引**，这称为index nested loop join或lookup join

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230215185355846.png" alt="image-20230215185355846" style="zoom:50%;" />

![image-20230215185459103](https://raw.githubusercontent.com/BoL0150/image2/master/image-20230215185459103.png)

join总结：

- 选小表作为外表
- 尽量缓存外表
  - **增加内表的缓存没有意义**，因为外表只需要遍历一次，而内表需要被反复地遍历（对外表的每一块，内表都要被遍历一次），所以不管内表的缓存有多大，内表的缓存总是会被刷新，IO次数不会改变。而内表的遍历次数是由外表的块数决定的，外表缓存越多，外表的块数就越少，内表的遍历次数就越少，就可以减少IO次数
- 遍历内表（或使用索引）

## sort-merge join

先给参与join的两个表**按照连接列的字段进行排序**（使用外部归并排序），然后对这两个排好序的表进行merge，伪代码如下：

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230215210850601.png" alt="image-20230215210850601" style="zoom:50%;" />

merge阶段维护两个指针，它们先分别指向R表和S表的第一行数据，然后比较这两个指针所指向的两行数据的连接列字段的大小，之后如上图所示走向不同的分支，但是这段伪代码其实少描述了一种情况：在某些时刻指针会有回溯操作（相应的场景在slides中有提及），DBMS的具体实现中是包含这一点的

sort的开销是外排序算法的开销，merge阶段的开销就是把两个表都遍历一遍的开销，总的开销就是它们加起来

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230215211204738.png" alt="image-20230215211204738" style="zoom:50%;" />

但是这种策略存在退化的问题，和指针回溯的操作有关，极端情况就是要join的两列里所有字段的值都相等，这会退化成最原始的Nested Loop join

在要参与join运算的表都是已经排好序的情况下（一种情况是join的下层的算子排好序了，一种情况是通过join key的索引来扫描表），merge join的效率是最高的，开销最低，硬盘IO次数只是两个表的页数之和。此外，如果我们期望join的结果是排好序的，那么merge join也非常合适，因为在这个算法内部实现里面已经完成了排序，在这种场景下使用别的join算法都还需要额外再执行一遍外排序

## Hash Join（index nested loop join）

在前面介绍index nested loop join时，是以B+树为索引来举例的，B+树点查询的时间复杂度是O(log N)，随着其中存储的KV增多，点查询速度会变慢，虽然B+树的好处是支持高效的区间查找，可以按照K递增/递减的顺序遍历叶子节点中的KV，但在index nested loop join场景下，我们要进行的是一次次的点查询，前后两次查询中的key大概率毫无关系，所以说我们也用不到B+树的这个特性。与B+树索引相对的是哈希索引，不管哈希表里存储了多少KV，哈希索引的开销始终都是常熟量级O(1)，点查询执行的飞快，这非常符合我们的期望，因此不妨将B+树索引替换成哈希索引

Hash Join的策略是给outer table构建哈希索引，对inner table进行遍历

- 阶段1 Build，构建哈希表
  扫描outer table R，使用哈希函数h1构建哈希表，以要join的字段为Key
- 阶段2 Probe，点查询
  去inner table S以哈希函数h1进行查询，如下所示，以内表的每一行中要 join的字段为key去阶段1中的哈希表里查询，如果在哈希表里找到了能match的KV，那就可以完成相应的join操作

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230215213214437.png" alt="image-20230215213214437" style="zoom:50%;" />

哈希表中每个KV中的Key是要join的连接列的字段，Value的选择也存在早物化/晚物化的差别：

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230215213548377.png" alt="image-20230215213548377" style="zoom:50%;" />

原始的哈希join中，每次都使用inner table中一行的join key去哈希表里查询，但使用有些join key去查询的时候，根本没有对应的哈希表项，这种无效的查询增大了开销，我们不妨使用布隆过滤器，给outer table构建哈希表的时候顺便构建一个布隆过滤器，inner table在去哈希表中查询前，先去查布隆过滤器，判断本次查询在哈希表中能否找到相应的表项，如果能通过布隆过滤器断定哈希表里没有对应的表项，便可以确定这是一次无效的查询，于是让此次查询提前结束

![image-20230215214759079](https://raw.githubusercontent.com/BoL0150/image2/master/image-20230215214759079.png)

如果给outer table构建的哈希表太大了（因为outer table太大），内存里放不下，那么我们就要把哈希表的部分内容驱逐到硬盘里，因此就有了相应的驱逐策略：

**Grace Hash Join**

做两套哈希表，也就是对前面的例子中的S表和R表都构建相应的哈希索引，并且使用相同的哈希函数。我们把哈希表存在硬盘里，查询索引时，把硬盘中两个表相对应的哈希桶都取出，因为我们使用的是相同的哈希函数，所以相同的（也就是可以match的）tuple所在的哈希桶号是一样的，之后我们对刚刚从硬盘里取出的两个哈希桶里的tuple做nested loop join（为什么需要对内表也构建一个Hash表？直接将外表构建的Hash表放入磁盘，遍历内表时对每一个tuple的关键字段进行Hash，就可以得到和这个tuple匹配的外表的tuple在Hash表的哪个桶中）

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230215220617999.png" alt="image-20230215220617999" style="zoom:50%;" />

这个策略提出的基础是认为：虽然无法把整个哈希表放进内存里，但可以把哈希表的某个哈希桶放入内存，如果单个的哈希桶太大，也放不进内存，那该怎么办呢？如果使用哈希函数h1构建的哈希表里的哈希桶太大，那我们就把这个大的哈希桶存储硬盘，使用哈希函数h2，对这个哈希桶再进行一次哈希，切分成更小的Hash桶，再将更小的桶放入内存

如下所示，最终我们会依次把下图的0号页，1'号页，1''号页，1'''号页,...,n号页读入内存，和S表的哈希桶的页来进行匹配操作。R表原来的1号哈希桶因为太大被再次哈希拆分成了三个小的哈希桶，在执行匹配操作时，S表的1号哈希桶也要再次以h2函数进行一遍哈希操作，拆分成更小的块，然后依次执行匹配操作

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230215221126323.png" alt="image-20230215221126323" style="zoom:50%;" />

grace hash join的开销如下（在输出join结果之前的开销）：

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230215221555262.png" alt="image-20230215221555262" style="zoom:50%;" />

- 构建哈希表阶段的开销是2(M+N)次硬盘IO，其中有(M+N)次IO是把R表和S表读进内存，因为哈希表的空间复杂度是O(N)，所以说给R表和S表构建的哈希表大约有(M+N)个页大，把这两个哈希表写入硬盘的IO开销便是(M+N)次IO，因此建立哈希表阶段总的开销是2(M+N)次硬盘IO
- 匹配阶段的开销是(M+N)次IO，因为我们只需把两个哈希表各自的哈希桶都读进内存，这一步没有考虑输出join结果的IO

如果DBMS知道outer table的大小，就可以为它构建静态的哈希表（也就是简单的，不可扩容的），否则就必须构建可扩容的哈希表

![image-20230215222009699](https://raw.githubusercontent.com/BoL0150/image2/master/image-20230215222009699.png)

**以上的开销都没有考虑将join的结果输出到磁盘上的IO次数**

如果是面对OLAP型的负载，join两个大表，那么大多数情况下哈希join最为合适，但也有一些特例，如果数据是“倾斜的”，也就是说为其构建哈希表会导致严重的哈希碰撞，sort-merge join效率好的多，此外，如果要求join的输出结果必须有序，sort-merge join也是最优选择，DBMS的优化器会结合实际场景在sort-merge join和hash join之间做选择

# 查询执行

执行计划通过算子构成的树实现，数据从叶子节点往上经过不同的算子处理，最终从根节点输出

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230216101051831.png" alt="image-20230216101051831" style="zoom:50%;" />

## Processing Models

DBMS的查询语句处理模型定义了数据库系统将如何执行一个查询计划，根据不同的工作负载（如OLAP/OLTP），它在设计上有一些权衡，有如下三种常见的计算模型：

DBMS计划执行/函数调用的方向分为两种，一种是让父算子调用子算子，自顶向下拉取数据，另一种是子算子完成操作之后调用父算子，自底向上推数据，但不论如何，数据流的方向始终是从操作符树的叶子节点流向根节点

### Iterator Model 迭代器模型/火山模型/流式模型

火山模型将关系代数中每一种操作抽象为一个 Operator，将整个 SQL 构建成一个 Operator 树，从根节点到叶子结点自上而下地递归调用 next() 函数。

每个算子/操作符要实现一个`Next()`方法，每次调用它时操作符会返回一个tuple或者null，null表示tuple已经都被返回完了。操作符本身由一个循环实现，循环内部调用其子操作符的`Next()`方法，每调用一次，子算子就会向它发射（emit）一个自己处理完的数据，从而可以从子算子中获取下一条数据供自己操作

![image-20230216102826909](https://raw.githubusercontent.com/BoL0150/image2/master/image-20230216102826909.png)

位于根节点的投影算子的实现方式就是循环地调用其子节点的`Next()`方法，然后将所有返回的tuple经过投影处理后输出。其子节点join算子的`Next()`函数的实现方法是（其实就是上个Lecture中的各种join算法里的hash join）：先循环地调用其左子节点的`Next()`函数，用所有返回的tuple（它们合起来就是outer table）去构建哈希表（此时等待join算子的`Next`函数返回的投影算子处于阻塞状态）

![image-20230216102858486](https://raw.githubusercontent.com/BoL0150/image2/master/image-20230216102858486.png)

之后循环地调用其右子节点（筛选算子）的`Next`方法，每次调用时，右子节点吐出一个tuple，然后拿着join算子拿着这个tuple的join key去刚刚构建的哈希表里查询，查看能否成功匹配，如果可以的话，那就返回一条join后的结果，即向其父算子（投影算子）吐一条数据

![image-20230216103022143](https://raw.githubusercontent.com/BoL0150/image2/master/image-20230216103022143.png)

几乎所有的DBMS都会使用火山模型或者是它的变种，并且火山模型有一些“流处理”的思想在内，上图中join算子的右子树中，S表的一个tuple被通过`Next`函数吐出后传递给了筛选算子，筛选算子在经过筛选后再把它传递给join算子，join算子经过join操作后再把它传递给投影算子。这个过程是一条一条数据向上流动的

Order By操作符需要等到子操作符把所有tuple通过`Next`函数返回之后进行排序，最后按顺序输出，因此在这个过程当中它和它的父操作符都处于阻塞状态

火山模型便于实现对输出的控制，比如说SQL语句中有"limit 100"这样的关键字，限制只输出100条数据 ，在火山模型下我们只需先流式地输出100条，然后让顶端的算子停止输出。我们不需要额外控制最底层的table reader（也就是读表的算子），只需要在操作符树的顶端控制数据的出口。但火山模型在性能上也有一些问题，每一条数据的传输都依赖函数调用，虽然函数调用的开销远小于硬盘IO，但如果要上千万条这样大量的数据向上流动，函数调用的次数将非常多，这会降低性能

### Materialization Model 物化模型

算子一次性读入全部要处理的数据，将得到的结果一次性地输出。

每个算子对应的函数的返回值是数组形式的。先通过调用它的子算子对应的函数来一次性地获取要处理的数据，将数据处理完后加入到数组中，一次性将这个数组（也就是所有的结果）全部返回给父算子。

![image-20230216105106383](https://raw.githubusercontent.com/BoL0150/image2/master/image-20230216105106383.png)

交易和事务对应的OLTP型数据库会使用这种模型，因为OLTP通常是点查询，只会涉及几条数据，每次子算子吐给父算子的数据不多，DBMS可以接受，如果是OLAP的负载，那么每次函数调用会返回过多的数据，DBMS无法承受

### Vectorized/Batch Model 向量化模型/分批模型

火山模型每获取一条数据就要经过一系列的函数调用，物化模型每次函数调用可以获取很多数据，它们有各自的优点和缺点，向量化模型是这二者中和的产物。向量化模型中，每个算子也有`Next`函数，但它返回的不是一条数据，也不是所有的数据，而是一批数据（tuple batch），这样可以减少函数调用的次数，从而降低开销

next函数每次返回的也是一个数组，但是此数组并不是所有的数据，而是有一个计数器，当数组的内容到了一定数量之后就直接返回，像这样分批进行传送

![image-20230216105454604](https://raw.githubusercontent.com/BoL0150/image2/master/image-20230216105454604.png)

这种模型对经常进行大数据分析的OLAP型数据库比较友好，既能做到向上层算子返回的数据量不是太大，又可以控制函数调用的次数与开销。

如果CPU支持SIMD指令集（比如说Intel的AVX指令集，这是CISC指令集所独有的一种指令），SIMD指令集又名向量指令，单指令多数据，即同一条指令，可以在很多的核对不同的数据进行相同的运算。所以可以将一组操作数送上CPU，然后一次性把这一组操作数要进行的运算同时完成（前提是这一组操作数要执行的运算是同一种，比如说都做加法），那么这将更适合向量化模型，因为向量化模型中子算子给父算子每次传递的是一批数据，这一批数据要进行同样的运算（比如说传送给筛选算子的话，执行的全是比较大小之类的运算），一个SIMD指令就能让负责接收数据的父算子一次性完成对这一批数据的操作，这从硬件层面上大大地加速了DBMS查询语句的执行



## Access Methods

这个部分讨论的是DBMS访问表的办法（也就是相关的算子读下图中的R表和S表的方法）

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230216110815674.png" alt="image-20230216110815674" style="zoom:50%;" />

### Sequential Scan 顺序扫描

简单地说就是一页一页地扫描，先从缓冲池中获取page，如果没有，就去硬盘里获取，把每一页读入内存之后就一条一条地开始扫描其中的数据，之后"do something"

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230216110922035.png" alt="image-20230216110922035" style="zoom:50%;" />

全表遍历式的顺序扫描往往是查询计划执行时的性能瓶颈所在，因此顺序扫描也有一些相关的优化策略

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230216111427977.png" alt="image-20230216111427977" style="zoom:50%;" />

Prefetching是说如果缓存池知道接下来要进行顺序扫描的话，它会在执行器尝试读数据之前把要读的数据预加载进入缓存池；

Buffer Pool Bypass是说在顺序扫描的时候不把当前扫描的页送入缓存池，而是在内存中另外使用一块区域，当执行器扫描完这块区域里的数据之后就把这块内存释放，这样做的好处是：顺序扫描时被扫描过的页以后会有很大概率不会再访问，不把它放入缓存池的话可以让其他需要被缓存的页继续呆在缓存池里，而不是过一段时间后因LRU策略被踢出

Parallelization是说我们可以让多个需要做全表顺序扫描的算子并行地以多线程的形式去访问表，比如说thread 1从头开始读，thread 2从中间开始读，etc.

上图中的后三个策略将在下面着重介绍：

- Zone Maps
  首先假设有如下的场景，某个线程想扫描全表找到val字段的值小于100的tuple，但当前被扫描的页里没有符合要求的tuple，那么对这个页进行的扫描就是无意义的，只会徒增开销

  <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230216111901466.png" alt="image-20230216111901466" style="zoom:50%;" />

  为了解决这种问题，我们不妨给表的每个页做统计信息（如下所示），统计所感兴趣的字段的最小值/最大值/平均值/总和/数目，将这样的metadata存放在硬盘的其他位置（**注意！这些metadata不能放在所记录的page本身中！否则需要把这个page读入内存才能知道metadata的内容**）。那么比如说当需要读表的操作符（table reader）需要读表然后选择符合条件的tuple时，可以先看每个页的统计信息，如果通过统计信息能推断出该页里没有我们想要的数据，那我们就不用把这个页读入缓存池里，然后直接去尝试访问表的下一个页，这便提升了顺序扫描的效率

  <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230216112345381.png" alt="image-20230216112345381" style="zoom:50%;" />

  但是这种方法的缺点在于：会占用额外的存储空间来存放metadata，并且修改了page的内容后还需要更新metadata的内容

- Late Materialization
  存储引擎为列存储的DBMS中，算子输出的数据可以不是整条tuple，而是tuple的record id。这种方法适用于列存，不适用于行存，因为行存是所有的tuple放在一起的，不方便分离数据，而列存是一列的数据放在一起的，更加方便。

### Index Scan 索引扫描

我们想筛选出val字段的值大于100的tuple，其实可以尝试构建索引而非暴力地全表扫描，这样明显更有效率

如果我们有多个谓词（筛选条件），那么我们该先使用哪个谓词相应的字段的索引呢？原则就是“使用了某个索引之后，经筛选剩下的数据越少，就先使用这个索引”，详见下图场景

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230216113616581.png" alt="image-20230216113616581" style="zoom:50%;" />

### Multi-Index/"Bitmap" Scan 多索引扫描

还是上面的例子，我们也可以用age字段的索引把age<30的人筛选出来，同时也用dept字段的索引把dept='CS'的人筛选出来，之后取它们的交集，再从交集里面选取country='US'的人，取交集的过程可以通过bitmap/布隆过滤器等实现

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230216113834812.png" alt="image-20230216113834812" style="zoom:50%;" />

## Modification Queries（修改表的查询语句）

前面讲的都是读表的方法，属于`SELECT`语句的内部实现方式，但还有好多其他种类的SQL查询语句，诸如`INSERT`,`UPDATE`,`DELETE`这些，它们会修改表的内容，它们的执行逻辑与`SELECT`完全不同，它们需要

- 检查约束（e.g. 如果不允许表的某一列存在重复的元素的话，就不可以向表中随便插入数据）
- 维护索引（e.g. 插入新的数据之后也要更新索引）

`UPDATE/DELTE`语句：

- 子算子会把要处理的tuple的id传递给上层负责完成更新/删除操作的父算子，然后父算子通过id找到相应的tuple，然后执行对应的操作。

- 必须要记住在执行本次的查询语句时操作了哪些数据

  比如：名为people的表有关于salary字段的索引，我们在数据库中要给所有salary小于1100的人工资加100

  <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230216124200466.png" alt="image-20230216124200466" style="zoom:50%;" />

  执行SQL语句时，上图中上方负责完成更新操作的算子会先把和当前tuple有关的索引删除，然后更新tuple，最后将该tuple重新插回索引。因此，当扫描到salary为999的Andy对应的tuple时，会先从索引里删除该tuple对应的索引项，然后更新tuple，再重新插入对应的索引项，那么问题来了，新的tuple里面salary字段的值是1099，因此会被插到作为索引的B+树的后方的叶子节点里，而当前遍历叶子节点用的“光标”（cursor）还在tuple原先的位置。也就是说，光标继续往前挪动的时候，会再次碰到这个已经被改过tuple，而且由于它的salary字段是1099，小于1100，因此还会再加100，这就引起了错误。

  所以说负责完成更新操作的算子应该记住它更新过了哪些数据

`INSERT`查询语句有两种执行方案，

- 子算子完成tuple的物化，然后将整个tuple传递给插入算子，插入算子只需完成最终的插入操作以及维护索引
- 子算子只将record id传给插入算子，然后在插入算子内部先要物化tuple，才能把完整的tuple插入表中，并维护索引

## Expression Evaluation

最后介绍表达式计算的问题，SQL语句中的谓词本质上就是表达式

一个通用的方法是解析SQL语句时为其构建如下以算子为节点的树形结构

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230216125731707.png" alt="image-20230216125731707" style="zoom:50%;" />

但是这个通用的方法在如下场景中会出现问题（SQL语句中?可以理解为待用户输入的参数）

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230216125834796.png" alt="image-20230216125834796" style="zoom:50%;" />

每次获得S表的一个tuple的value字段的值后，都会计算用户传来的parameter和SQL语句里的constant的和，这会导致随着不断地扫描S表中的tuple，这个加法被重复地进行了好多好多遍，这大大地降低了效率（和朴素斐波那契解法中时间复杂度爆炸是同一个原因）

这种情况下存在对应的优化策略：在执行SQL语句的查询计划前，直接把表达式算出来，提取成一个常数

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230216130140566.png" alt="image-20230216130140566" style="zoom:50%;" />

这和Java的高级技术JIT（即时编译技术）有相似之处，和JIT（Just in Time）对应的是AOT（Ahead of time）

java是将用户的java语言先转换成字节码在虚拟机上运行，然后宿主机再运行虚拟机的二进制代码。而在Java的JIT技术中，JVM虚拟机会将被频繁执行的热点代码段（比如说JVM能检测出来有些循环被执行了好多好多次，那这就可以被标记为热点代码段）中的字节码转化成二进制代码，下次再运行到这个热点代码段的时候就直接运行二进制代码，不经过中间那层虚拟机，从而提升了效率

JIT技术是在代码被执行的时候动态地判断出哪段代码是频繁被执行的，而AOT技术是在代码被执行之前就进行这样的判断

# 查询执行-part2

本Lec关于query的并发执行

查询语句的并行执行会提升DBMS的吞吐量（DBMS在单位时间内能执行完多少SQL语句），以及单条SQL语句的延迟（即单条语句的执行时间），从而给上层业务提供更好的服务

- 单节点并行的DBMS：资源都是在一块的（比如说多个线程都在同一个机器里面），资源之间的通信速度很快，简易且可靠（比如说多线程通过共享内存通信）
- 分布式DBMS：节点之间有可能间隔的很远（在世界上两个角落），节点之间的通信依托于网络而非内存，并且节点之间通信的问题与代价无法被回避

本Lec将探讨单节点上并行运行的DBMS的机制

## Process Models

DBMS的process model描述的是多个用户的查询请求是如何并发执行的，一个worker可以理解为数据库的一个组件，用来执行一部分工作，比如说查询语句相应的执行计划可以被分解为多个部分，每个部分给一个worker去执行

process model有三大方向：

1. Process per DBMS Worker，每个worker一个进程
   每个worker给它分配一个OS级别的进程去做，这依赖于操作系统对该进程的调度，两个worker之间可以通过共享内存的方式进行通信，这种模型有一个优点：单个进程的crash不会让整个系统宕机。比较古老的DBMS通常会采取这种模型。

   这种模型的工作流程如下：业务应用/客户端的请求传过来之后，分配器会把请求切成多份工作，每份分配给一个worker，每个worker负责对数据进行操作，完成工作之后把结果返回给客户端

   <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230216145111935.png" alt="image-20230216145111935" style="zoom:50%;" />

2. Process pool，进程池
   Process per DBMS Worker模型下，由于系统中可能会有过多的进程从而降低性能（主要体现在进程反复创建和销毁？），因此有了池化的思想：每个 Worker 可以使用 Worker Pool 中任意空闲的进程，其调度依然依赖于操作系统的调度器。当请求到达时，分配器会在进程池中寻找可用的进程，把工作分配给它。用户请求通过Dispatcher分配相应的 Worker 来完成查询，并将结果返回给Dispatcher，后者再返回给用户
   这有利于控制进程的数量，减少对系统资源的占用

   <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230216145427146.png" alt="image-20230216145427146" style="zoom:50%;" />

3. Thread per DBMS Worker，每个worker一个线程
   随着pthread库这种跨平台的创建与管理线程的接口的诞生，现代DBMS便开始使用thread per worker这种process model，每个worker对应一个线程，DBMS只由一个进程组成，但会分成多个worker线程去运行，线程之间的调度由DBMS负责
   这个线程模型也存在一些问题：一个线程的崩溃会导致整个进程进而崩溃，整个DBMS也挂了，因此它的稳定性可能就不如前面process per worker的进程模型

   <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230216145722151.png" alt="image-20230216145722151" style="zoom:50%;" />

线程模型的优点也很明显：在一个进程内部的线程之间的切换带来的开销要比进程之间的切换的开销小得多，而且同一个进程的多个线程是天然地共享内存的（因为使用的是相同的页表），不需要像进程间通信那样人为地去管理共享内存
DBMS的多线程模型并不是表明会把单条SQL语句分成多个部分并发运行，而是说会同时有多个线程并发地去执行多个SQL语句

## Execution Parallelism

Inter-Query指的是查询之间的并发处理，

- 如果所有的查询都是read-only的，那就不容易产生并发导致的数据竞争，线程间的同步操作不多；但如果并发的查询都在更新数据库，那么这就是另一大问题，涉及到事务，并发控制，会在后面的Lecture介绍

Intra-Query指的是查询内部的并发处理，也就是把单条SQL语句分成多个部分并发运行：

- 单个查询内部的并行执行会提升查询的效率，它的设计思想类似于生产者-消费者模型，成熟的DBMS中，每个算子都有并发的版本（比如说并发的table reader和hash join），并发算子的实现有两大思路：第一个思路是多个线程去操纵集中的全部数据（比如说多个线程同时读一个表），第二个思路是把集中的数据切开，把每部分分给相应的线程，使得线程可以在本地处理

如果把并行执行模型看作是cpu执行指令：

- Inter-Parallism就像是流水线，把一整个指令切分成多个部分由不同的硬件执行，有些硬件做取指令，有些硬件做解析指令，有些硬件做运算
- Intra-Parallism就像是SIMD，将数据切分成多个部分，多个核对不同的数据执行相同的运算；
- Bushy就是今天的cpu，每一硬件负责一个阶段的事情，而每个阶段有大量硬件同时干同一件事。相当于超标量，也就是多流水线（注意！不是超流水线！超流水线是指细化流水线，提高主频）

Intra-Query有三种实现方法

1. 水平切分执行计划树
   把要处理的数据切开，每一部分给一个线程，且每个线程做的工作是一样的，DBMS还要在执行计划中插入一些exchange算子，用于做数据的聚集、拆分、重分布

   如下所示，以筛选算子的执行为例，**DBMS用exchange算子把数据切分成三个部分，每个部分由一个worker/线程去负责，exchange算子负责去并发地调用这三个部分的`next`方法**，从而达到并发地去读表的目的

   <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230216152450884.png" alt="image-20230216152450884" style="zoom:50%;" />

   exchange算子上方可能会有其他的算子，比如说投影算子这种，它会一次一次地调用exchange算子的`next`方法，**当exchange算子的`next`方法被调用时，exchange算子会并发地调用它的多个worker**，然后将这些worker返回的数据聚集之后返回给上层的算子

   exchange算子不仅存在于多线程的DBMS，一些分布式DBMS中也有，它包含三大类，如下所示

   <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230216152734149.png" alt="image-20230216152734149" style="zoom:50%;" />

   下图是使用exchange算子的另外一个例子，做的是hash-join

   <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230216152817957.png" alt="image-20230216152817957" style="zoom:50%;" />

   用三个线程并行地构建哈希表，之后用另外的三个线程去并行地进行点查询，并且用exchange算子把并行计算的结果汇聚起来

2. 垂直切分执行计划树
   垂直切分是说把执行计划树中的**每个算子丢给一个相应的线程/worker去执行**，它们之间是并发执行的，同时算子之间也有数据的传递，这和现代处理器中的流水线很像

   <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230216153238837.png" alt="image-20230216153238837" style="zoom:50%;" />

3. Bushy Parallelism，将前两种方法结合
   既做到将算子切开并发执行，也做到算子和算子之间并发执行，并且依然需要exchange算子
   结合如下场景分析，要做A~D四个表的join，thread 1做A join B，thread 2做C join D，这便是算子之间的并行执行，属于对查询计划的垂直切分
   按理说应该把thread 1和thread 2的输出结果丢给一个join算子，但如下图所示，bushy模型下会将这个join算子的内部执行切开完成，也就是把中间结果切成两半，一半丢给thread 3，一半丢给thread 4，**这两个线程的操作逻辑是一样的，就是负责的数据部分不同**，这属于对查询计划的水平切分，并且其中的切分和汇聚都依赖exchange算子来完成

   <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230216153455684.png" alt="image-20230216153455684" style="zoom:50%;" />

15-445针对的是基于硬盘的DBMS，因此大部分情况下硬盘I/O是性能瓶颈，前面所介绍的基于并行的优化策略也都是基于“数据已经被读到了内存中”的假设，如果数据无法及时从硬盘读入内存，再多的优化也是作用有限的。并且如果每个worker在尝试读硬盘的不同的部分的话，事情会变得更糟糕，因为这会导致对硬盘的频繁的随机访问，因此接下来会分析对硬盘I/O的性能优化

## I/O Parallelism

如果把数据库的数据切成不同的部分，存到不同的存储设备中，这样在对数据库进行并发存取时就会做到在硬件层面上隔离开，从而通过对多个硬盘的并发读写提高DBMS的性能。

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230216153853403.png" alt="image-20230216153853403" style="zoom:50%;" />

实现I/O并行有如下具体途径：

### Multi-Disk Parallelism（RAID）

**多磁盘并行对于DBMS是透明的，它会通过对 OS 或硬件的配置将 DBMS 的数据文件存储到多个存储设备上**，但是用户看起来还是像在一个磁盘上。

比如说它可以借助存储硬件的RAID技术，**RAID 0机制会将文件分成多份存入不同的磁盘**，这样**在读写文件时可以进行并行的读写**

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230216154415358.png" alt="image-20230216154415358" style="zoom:50%;" />

RAID 1机制会将每个硬盘做成彼此的镜像，这样可以避免由于单个硬盘的损坏导致数据的丢失，同时也可以在读文件数据的时候并行地从每个硬盘读取同一个page的不同部分，提升了读文件的效率。缺点是存储空间利用率低

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230216154829763.png" alt="image-20230216154829763" style="zoom:50%;" />

此外RAID还有其他的等级，比如说RAID 5中可以把文件数据拆分到两个硬盘存储，第三块硬盘存储的是前两块硬盘里的数据按位XOR的结果，这样的话如果前两块盘里有一个损坏，可以通过第三块盘来恢复数据，这就结合了RAID 0和RAID 1的优点

### Database Partitioning

这是关于将DBMS所存储的数据切分成多个部分的策略

**分库**

我们可以把数据库实例中的不同Database分配到不同的硬盘当中，还可以在文件系统层面将不同的Database存入不同的文件夹中

**分区**

我们可以对单个的表进行分区，在物理上将它分成多个部分，每个部分落在不同的盘上，从而提升I/O的性能。对表进行分区的方法有如下几种：

- Vertical Partition
  垂直分区，如下所示：

  <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230216155927319.png" alt="image-20230216155927319" style="zoom:50%;" />

  <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230216155947796.png" alt="image-20230216155947796" style="zoom:50%;" />

  而且如果表中有几个不常用的属性，而且这些属性占用的存储空间特别的大（比如上图中的attr4），就可以使用这种垂直分区策略。因为这些又长有冷的数据经常读写的话开销会比较大。

- Horizontal Partitioning
  水平分区，如下所示：

  <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230216160151678.png" alt="image-20230216160151678" style="zoom:50%;" />

  <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230216160219807.png" alt="image-20230216160219807" style="zoom:50%;" />

# Lec15-并发控制理论

接下来将介绍DBMS的事务与并发相关问题

如下所示的层次图并不代表着DBMS实现的所有组件，

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230305175858404.png" alt="image-20230305175858404" style="zoom:50%;" />

除此之外DBMS还有两大组件横跨了多个层级：并发控制，数据库恢复

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230305180024124.png" alt="image-20230305180024124" style="zoom:50%;" />

- 并发控制是想避免数据竞争的发生
- 数据库恢复是想在实现持久化时一直保持正确的数据库状态：假设银行业务中想实现“从A账户转100元给B账户”，但如果A账户扣100元这个操作被写入磁盘之后机房断电了，如果当机器恢复过来时没有采取一定的恢复手段使得B账户加100元，那么就出现了错误（这和xv6文件系统通过日志实现事务的目的是相同的，都是想使得机器的crash不影响系统的状态的正确性）

数据库的并发控制与恢复都是基于事务的ACID理论，首先介绍事务

事务是一系列的操作（e.g. 可以是**一系列的SQL语句**），DBMS通过执行这一系列的操作从而达成更高级的功能（比如说前面提到的转账的问题就属于事务）。并且事务也是DBMS执行的最基本单位：**DBMS不允许只完成部分的事务，只存在两种可能：事务彻底完成/事务没发生**。

一个事务的例子：

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230305180427905.png" alt="image-20230305180427905" style="zoom:50%;" />

**事务是数据库操作的最基本的单位**（所以这上面的三条语句要么都成功，要么都失败），我们平常使用数据库的时候都是直接写sql语句，此时没有告诉数据库要开事务，所以单条sql语句就是一个事务。

一个很“想当然”的想法

- 到达DBMS的事务被串行处理，在事务开始之前把数据库文件备份一份，在这个复制得到的文件上执行事务，如果事务成功执行，那么把原本的数据库文件删除即可；如果执行失败，那么把复制的文件删了，回到之前备份的文件就行了
- 这个策略理论上可行，但不可以实践，因为如果数据库文件很大的话，再复制一份的话磁盘可能放不下；并且彻底的串行会导致极差的性能，彼此之间无关的事务是可以并发的

那么DBMS该如何实现事务的并发呢？

显然不可以放任所有的事务直接去并发执行，这样的话，它们的内部操作彼此之间互相交错地被执行，事务之间互相影响，很可能导致数据库状态的错误

为了便于分析问题，我们使用下面的简化后的模型

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230305182434274.png" alt="image-20230305182434274" style="zoom:50%;" />

对于DBMS使用者，比如说负责后端业务的程序员，想要实现事务，可以通过如下的SQL关键字：

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230305182612787.png" alt="image-20230305182612787" style="zoom:50%;" />

实现事务正确性的标准是ACID，其具体介绍如下：

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230305182833102.png" alt="image-20230305182833102" style="zoom:50%;" />

- 原子性：所有操作要么都执行，要么都不执行，如果操作中间被打断了，数据库要负责把之前的操作给撤销
- 一致性：可以理解为事务结束之后，系统中的invariants不会变化，比如A转给B100元，转之前和转之后AB账户的总和应该是一样的，没有改变物理规律
- isolation（隔离性）：一个事务在执行时，其他的事务对数据库数据的更改它是看不见的，它对于数据的更改也不会被其他事务看到，直到它commit了之后，其他的事务才可以看见它的更改
-  持久性：事务如果提交了，就永远存在

## Atomicity

一个事务的执行只会有两种结局，一是完成全部的内部操作之后执行commit，二是中途被abort，可以是客户端主动发出的中止，也可以被DBMS中止，这会形成回滚的效果，撤回该事务之前做的所有操作，相当于事务还没开始执行

想要实现事务的原子性，有如下手段：

- Logging，日志：事务在执行的过程中，每执行一步都要记录一下“如果要回滚这一步该怎么办”，就相当于在走路时，每走一步就留下一个记号，这样便可以原路返回。这种日志也被称为"undo log"-回滚日志，内存和磁盘中都会有它的备份，它就像是飞机的黑匣子
  这样的话，当事务执行到一半，用户发起回滚或者DBMS决定开始回滚时，DBMS就会依据留下的log一步一步地把所有执行过的语句所造成的更新撤回

  日志还有其他的好处：

  - 可以在业务出错时拿来复盘分析用
  - 也可以提升性能，因为用户的操作往往是随机的读写，我们可以在需要对文件数据进行写操作时，先写入内存里的日志，而不是基于业务需求直接进行随机的磁盘读写（但是需要将日志同步写回磁盘），因为立即写回磁盘会让用户在有些时候被阻塞住。可以通过日志采用延迟写回的方式，在之后合适的时间将记录在日志中的操作写回磁盘

  xv6文件系统中的日志系统就是undo log，在事务中如果涉及到对文件的写，那么会通过`log_write`函数先写入缓存池和内存里的日志缓存，直到事务完成后向磁盘提交日志，然后根据被写入磁盘的日志文件install日志，这样的话，事务内部对文件的写就不用在落盘后才能返回，而是写入内存中的缓存池和日志缓存后就可以返回，不用让用户被缓慢的磁盘IO阻塞

- Shadow Paging

  事务想修改哪些文件页，DBMS就给这些页做备份，事务操作这些备份，如果事务成功提交，那就拿备份替换下原有的页，否则就删除备份，实现回滚。也叫增量备份，与git类似，每次commit只在本地新增修改的文件，与之前的commit共享没修改的文件。

  现代数据库中使用这样的策略的并不多，绝大部分都是使用日志实现原子性

## Consistency

数据库中的所反映出的外部世界应该是逻辑上正确的，而且我们对数据库所执行的查询的结果也是逻辑上能讲的通的（A给B转100块钱，A扣100，B加100，A和B的总和不能变）。笔者认为，这也可理解为，事务中的一系列内部操作结束之后invariants不能被破坏

一致性分为如下两方面：

- Database Consistency，数据一致性

  后面发生的事务要能看到在它之前发生的事务导致的结果

- Transaction Consistency，事务一致性

  这是由业务去保证的

## Isolation

理想情况下，事务之间的隔离可以做到用户在执行一个事务的过程中，好像数据库没有其他的用户在使用（这和操作系统的虚拟化类似），无法看见其他的事务**在当前事务开始执行后**对数据库进行的任何修改，不管这个修改有没有被其他事务提交，能看到的只有事务开始时数据库的状态

实现隔离性的好处在于可以给负责写业务的程序员提供很好的抽象和便利，“A给B转100元”这个业务里只需要考虑给A扣100，给B加100，写这个业务的时候可以认为“只有我这一个用户在操纵数据库”。如果没有实现隔离的话就要被迫考虑很多其他的事情，比如说“有没有其他人在操纵B的账户”，“B的账户余额是否达到了上限”，实现了隔离之后，业务程序员只需写好事务对应的SQL语句然后commit，如果确实存在着前面的问题，事务就会失败，然后就会回滚重做

为了实现事务的隔离性，就需要数据库的并发控制机制去决定多个事务之间的交错的执行是以怎样的一个顺序/时间表来进行，不可以随意地交错执行。实现并发控制有两大流派：

- 悲观协议：不要让问题发生，在问题出现之前就让线程停住
- 乐观协议：我们假设并发的冲突是少数的，只在问题出现之后再去回滚

举个例子分析，假设有如下的业务场景，T1和T2是两个业务里面实现的事务

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230305202155683.png" alt="image-20230305202155683" style="zoom:50%;" />

如果T1和T2同时提交，我们无法保证T1和T2谁先执行，但是隔离性要求**最终的结果应该看起来像是两个事务串行执行的**，也就是说如果正确的情况下，最终只允许出现两种执行结果

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230305202754831.png" alt="image-20230305202754831" style="zoom:50%;" />

输出的结果取决于两个事务谁先执行，但是一致性要求AB加起来的和是不变的，

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230305202852075.png" alt="image-20230305202852075" style="zoom:50%;" />

因此，如下所示，两个事务直接串行执行是没有一致性的问题的

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230305202950538.png" alt="image-20230305202950538" style="zoom:50%;" />

但是不能真的让这两个事务完全串行，因为这会影响性能。如果这两个事务是并发执行的，那么该如何让它们在保证一致性的情况下交错执行？

在如下所示的场景下，并发事务的交错执行就出现了一致性问题：

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230305203231785.png" alt="image-20230305203231785" style="zoom:50%;" />

上图的一系列操作，在DBMS的视角下，其实是进行了如下的一系列读和写

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230305203510631.png" alt="image-20230305203510631" style="zoom:50%;" />

那么我们该如何实现一个算法，让DBMS能够判断出一系列的读写是否会导致一致性的错误？

一系列操作的执行顺序被称为数据库系统的执行调度，**两个输出结果相同的执行调度被称为是等价调度**。**如果一个执行调度能够和真正的串行执行等价，那么它就拥有正确的一致性，它也被称作可串行化调度**（Serializable Schedule）。而且如果每个事务都保证了一致性的话，那么每个可串行化调度也都可以保证一致性。如下图，左边的就可以等价于串行调度，所以结果保持了一致性

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230305204151912.png" alt="image-20230305204151912" style="zoom:50%;" />

**那么应该如何判断一个执行调度是可串行化调度呢？如果实现了这一点，DBMS就可以判断出某个调度是否会导致一致性的错误**

首先，我们需要实现一个算法去证明两个执行调度是等价的，这就需要引入如下的新概念：冲突操作（"conflicting" operations）：

- 如果两个操作来自不同的事务，它们都在操作同一个数据并且至少其中一个是操作是写，那么这两个操作就是冲突的。

  <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230305223217214.png" alt="image-20230305223217214" style="zoom:50%;" />

  - R-W冲突：

    <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230305223540197.png" alt="image-20230305223540197" style="zoom:50%;" />

    这破坏了隔离性，又被称为“不可重复读”，也就是重复读的结果和之前读的的结果不一样

  - W-R冲突：

    <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230305223940534.png" alt="image-20230305223940534" style="zoom:50%;" />

    脏读，也就是一个事务读到了另一个事务未提交的数据，并且基于这个数据做了其他的操作，这个也是因为破坏了隔离性导致的。比如上图，T2事务是基于T1事务的还没提交的中间结果进行的，如果T1最终Abort了，T2的结果就会出问题

  - W-W冲突：

    <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230305225027433.png" alt="image-20230305225027433" style="zoom:50%;" />

    如果按照隔离性的要求，要求两个事务看起来是串行执行的，只会出现两种结果，一种是10和Andrew，一种是19和Lin。但是上图的做法破坏了隔离性，T2在T1的中间结果上执行，导致覆盖了T1先写的值，最后的执行结果是19和Andrew，破坏了一致性

  冲突操作可以用来检测一个操作调度是否是正确的，但是不能用来生成一个正确的调度

关注冲突操作的意义在于：两个事务中冲突的操作不能在时间序列上交换位置，否则会改变执行调度的结果。接下来会基于上面的这三种冲突，介绍判断一个调度是不是可串行化的方法

可串行化分为如下两个等级：

- Conflict Serializability，冲突可串行性/基于冲突的可串行判断
  这涉及一个新的概念：冲突等价（conflict equivalent）
  如果两个执行**调度**包含了相同事务的相同操作，并且有相同的冲突，并且输出结果相同，那么这两个调度就被称为冲突等效的

  如果某个执行调度S和某个真正串行的执行调度冲突等效，那么它就是冲突可串行化的（conflict serializable）

  如下的调度就是一个冲突可串行化调度

  <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230306111247205.png" alt="image-20230306111247205" style="zoom:50%;" />

  根据上面的判断规则，我们可以交换两个事务中连续且不冲突的操作的位置，像下面这样：

  <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230306111416476.png" alt="image-20230306111416476" style="zoom:50%;" />

  <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230306111458250.png" alt="image-20230306111458250" style="zoom:50%;" />

  <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230306111527232.png" alt="image-20230306111527232" style="zoom:50%;" />

  这个基于swap的算法对于仅包含两个事务的执行调度很有效，但如果调度中有很多事务，这个算法的开销会变得巨大，并不适合

  因此后来就有了如下的基于依赖图的算法：
  如果一对冲突操作中`Ti`事务里的`0i`操作先于`Tj`事务里的`0j`操作执行，那么就在依赖图中画从`Ti`指向`Tj`的一条有向边，

  <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230306111729621.png" alt="image-20230306111729621" style="zoom:50%;" />

  如果依赖图中没有环，就说明该调度可以冲突串行化，如下图的调度就可以变成串行化的调度，按照T2，T1，T3的顺序串行执行

  <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230306112102479.png" alt="image-20230306112102479" style="zoom:50%;" />

  如果依赖图中形成了环，就表明该调度不可冲突串行化，就说明该调度会出问题。如下图，形成环之后，T1要在T2之前执行，T2要在T1之前执行，无法冲突串行化

  <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230306112832830.png" alt="image-20230306112832830" style="zoom:50%;" />

  但是有可能只修改业务逻辑使得该调度即使是无法冲突串行化，但是结果仍然是正确的

  <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230306113350276.png" alt="image-20230306113350276" style="zoom:50%;" />

  因为在执行调度开始之前B一定是非负的，T1事务中对B的行为是让B增加，所以为了串行化哪怕强行让T1的`W(B)`在T2的`R(B)`之前执行，都不会改变`if(B>=0)`的判断结果，无论如何`cnt++`都会被执行

基于观察的可串行性说的就是通过观察来确定某个执行调度是可串行还是不可串行，它与冲突可串行性相比，对执行调度的要求要更加宽松，但目前还没有DBMS能实现它。

基于观察的可串行化和基于冲突的都没法做到不错杀

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230306114429696.png" alt="image-20230306114429696" style="zoom:50%;" />

## Durability

事务的持久性要求事务提交的所有更改必须被写入存储介质持久化，并且不能有更新只进行一半的情况，也不能有事务失败之后更新被残留的情况

DBMS通过使用前面提到的logging或shadow paging手段实现这一点

# Lec16-两阶段锁

在上一个Lecture中所介绍的判断执行调度是否满足冲突可串行化的方法需要等到执行调度里所包括的多个事务都执行完之后才能判断出这个执行调度是否可串行化，但等到执行调度所包括的事务全都执行完并提交了之后，它们就都已经完成了对DBMS的更新，就算我们检测出来这个执行调度是不可串行化的，但是DBMS中的数据已经被修改了，一致性已经被破坏，因此这种检测可串行化的方式是一种马后炮的方式，是无济于事的

相应的解决方法是我们用锁来保护数据库中的被共享的对象，从而避免数据竞争导致，进而避免R-W/W-R/W-W冲突，因此Dependency Graph中就可以减少相应的有向边，进而无法成环，执行调度就可以从不可串行化变为可串行化。一个朴素的加锁的方法如下：

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230306134207478.png" alt="image-20230306134207478" style="zoom:50%;" />

在事务T1访问A之前，先通过DBMS的锁管理器（Lock Manager）获取A的锁并且注册（记录下来“A的锁当前归T1所有”），之后事务T2想访问A，于是也要获得A的锁，锁管理器便会拒绝它的请求，T2之后便阻塞在这里，直到T1完成了对A的全部操作后通过锁管理器释放A的锁，T2才可以通过锁管理器获取A的锁，并且完成对A的全部操作后释放A的锁

## Lock Types

老生常谈，Lock&Latch的区别如下：

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230306134709495.png" alt="image-20230306134709495" style="zoom:50%;" />

Latch保护的是具体的数据结构中的数据，Lock保护的不是具体的数据结构，而是数据库的抽象的内容，比如说向锁管理器申请的可以是对数据库的表的某一行的锁，这个锁会保护涉及这一行的所有的索引里面关于这一行的部分

一般在实际场景中都是先获取Lock，即我们要操作的逻辑对象的锁，对这个逻辑对象内部的数据结构实际进行操作时再获取Latch

Lock和Latch一样，都有两种类型的锁：

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230306134944530.png" alt="image-20230306134944530" style="zoom:50%;" />

- S-Lock，共享锁
  和Latch中的读锁差不多
- X-Lock，排他锁
  和Latch中的写锁差不多

在带有Lock的情况下，事务执行的过程如下：

- 事务获取对应的锁
- 锁管理器授权或阻塞事务
- 事务释放锁

锁管理器内部的数据结构（lock-table)记录着锁的情况，它追踪着哪个事务获取了哪个锁，哪个事务在等待哪些锁

如下的场景中就使用了X/S这两种Lock：

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230306135656502.png" alt="image-20230306135656502" style="zoom:50%;" />

那么通过加锁，能将原本不可串行化的执行调度的执行结果变成正确的吗？如下所示，可以发现，**T1事务和T2事务还是没有隔离开**，**脏读和不可重复读的问题还是没有解决**，导致调度的执行结果依旧不具有一致性

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230306135744752.png" alt="image-20230306135744752" style="zoom:50%;" />

因此，后来的DBMS方面的学者专家们就在此之上进行改进，提出了两阶段锁来实现并发控制

## Two-Phase Locking

二阶段锁是一个并发控制协议，它规定了一个事务在运行的过程中如何跟其他事务之间协调锁，从而实现可串行化。使用两阶段锁不需要提前知道完整的执行调度，它会在调度进行的过程中避免不可串行化的情况发生

二阶段锁中有两个阶段

- 增长阶段（Growing）
  在这个阶段事务只能不断地获得锁，不能释放锁
- 缩小阶段（Shrinking）
  在这个阶段只能释放释放锁，不能再获取新的锁

在**一个事务**的生命周期里，它所持有的锁的数量的增长趋势如下所示，在增长阶段结束后，事务不允许再获取锁，最后将所有获取过的锁都释放后会提交事务：

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230306140840923.png" alt="image-20230306140840923" style="zoom:50%;" />

使用二阶段锁便可以使得不可串行化的执行调度的最终执行结果具有一致性，如下所示，在两阶段锁协议下，事务T1执行完`W(A)`后并不会立即释放A的锁，因为二阶段锁协议的规定就是“**在一个事务中，先一直获取各个锁，然后把所有获取的锁逐个释放**”，直到`R(A)`执行完了之后T1才会释放锁（如果按照之前的策略，先获取X-Lock，再释放X-Lock，然后再获取S-Lock，之后再释放S-Lock，这就违反了两阶段锁的协议）

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230306143317404.png" alt="image-20230306143317404" style="zoom:50%;" />

在使用了二阶段锁协议后，相应的执行调度对应的依赖图（Dependency Graph）一定没有环，二阶段锁可以严格地保证冲突可串行化

二阶段锁解决了不可重复读的问题，因为T2只有在T1释放了锁之后才能获取锁对数据进行修改，而由于二阶段锁的要求，此时T1已经不能再获取读锁再次读取该数据了

但是二阶段锁没有解决脏读的问题，在某一个事务释放了所有锁但是还没提交的时候，其他事务仍然有可能读到该事务的中间结果

如下所示，T1释放锁之后，T2事务开始被执行，T2对A的操作是基于T1对A进行临时修改后的版本进行的，**如果T1事务没有提交而是被abort了，那么T2必须跟着T1一起回滚**，此行为被称之为“级联回滚”（如果T2进行的是读操作，那么这也被称为脏读，"dirty reads"）

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230306162414687.png" alt="image-20230306162414687" style="zoom:50%;" />

级联回滚本质上的原因是T2事务在T1事务更新得到的临时版本的数据上进行了操作，那我们可以通过一些手段让T2不在T1修改得到的临时版本上进行操作：比如说，可以让事务先获取各个需要获取的锁，**等到它commit时再一次性把这些锁释放掉**，也就是对事务所修改的数据在整个事务中全程加锁，这样的话，T2就不可能在临时版本上进行操作。这个方法也被称为严格二阶段锁（Strong Strict 2PL，简称SS2PL）

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230306162911229.png" alt="image-20230306162911229" style="zoom:50%;" />

严格二阶段锁协议的特点是事务所修改的数据在事务结束之前，其他事务都不能读写（因为所修改的数据在事务中全程加锁），这个协议的好处就是不会产生级联回滚。

举个例子，事务T1是A给B转账100，事务T2是计算A和B的账户余额的和

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230306163930512.png" alt="image-20230306163930512" style="zoom:50%;" />

如果完全不使用二阶段锁，那么就有可能像下面这样出现不一致

![image-20230306164012115](https://raw.githubusercontent.com/BoL0150/image2/master/image-20230306164012115.png)

如果使用了二阶段锁协议，那就可以保证一致性，等效成T1事务先执行，然后T2事务执行。但存在潜在的级联回滚问题

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230306214107773.png" alt="image-20230306214107773" style="zoom:50%;" />

如果使用了严格二阶段锁协议，如下所示，既可以保证一致性，也可以避免级联回滚

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230306214224941.png" alt="image-20230306214224941" style="zoom:50%;" />

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230306214536018.png" alt="image-20230306214536018" style="zoom:50%;" />

不会级联回滚的执行调度和基于强二阶段锁协议的执行调度对应的集合如图中所示

## Deadlock Detection + Prevention

结合如下的执行调度的场景进行分析，事务T1的业务逻辑是先操作A再操作B，事务T2的业务逻辑是先操作B再操作A（死锁必要条件之环路等待），因此它们获取锁的顺序不同，并且**由于是二阶段锁协议，一个事务中获取另一个锁之前不允许释放当前锁**（死锁必要条件之请求并保持），就导致了下图的死锁

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230306215228212.png" alt="image-20230306215228212" style="zoom:50%;" />

所以2pl有可能会导致死锁

为了解决死锁的问题，DBMS有如下两种方案

- Deadlock Detection，死锁检测

  DBMS内部会维护一个锁等待图（waits-for graph），它记录了当前所有并发的事务（一个事务就是一个线程？）里谁在等谁的锁，图中每个节点对应一个事务，每条有向边对应一个锁的等待关系（从`Ti`指向`Tj`的有向边代表着事务`Ti`等待`Tj`释放一个锁），DBMS会周期性地检查这个图，看看图里有没有成环，如果有的话就会想办法把环给解开

  <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230306215759965.png" alt="image-20230306215759965" style="zoom:50%;" />

  锁等待图中已经成环了，已经达成了死锁的三个必要条件，资源互斥访问，锁的请求并保持，锁的环路等待，要想破除死锁，就只能从最后一个必要条件入手：不可剥夺。

  如果DBMS检测到锁等待图里出现了环，那就会选择一个victim事务，让它回滚，剥夺它的资源，这也就是破除了死锁的必要条件之不可剥夺。这样环就会解开，死锁被拆除

  被选择的victim事务要么会重启要么会中止，这和它是怎么被调用的有关：如果这个事务是DBMS用户的业务的一部分，就可以把它abort，因为用户的业务代码里会有一些应对abort情况的逻辑（比如说转账的事务进行到一半然后被abort，那么就会在前端告诉用户“转账失败，请稍后再试”）

  并且这个策略里有一些trade-off，因为DBMS是周期性地检查锁等待图，

  - 如果周期的频率很高的话，处理死锁的开销就比较大，因此不易检查地太频繁；
  - 但频率太低也不好，这有可能导致一些陷入死锁的事务被卡了好久

  不光是死锁检测的频率要做trade-off，选哪个事务当victim也要权衡，

  - 我们可以综合考虑“这个事务已经执行了多长时间”（让一个已经执行了很长时间的事务回滚，这不太合理）
  - “事务执行了多少”（可以以每个事务都执行了多少条SQL语句这样的指标来衡量）
  - “这个事务已经得到了多少个锁”（DBMS倾向于让得到的锁多的事务回滚，因为得到的锁越多，就有可能让更多的其他事务陷入阻塞，这样的事务回滚了之后其他事务就都能继续往下执行了）

  那么，选好了victim之后，我们该如何回滚事务有如下两个方案：

  1. 完全回滚，让victim事务回滚到它开始执行时的状态，就好像它没发生过
  2. 最小化地去回滚，去判断到底是哪几个SQL语句造成的死锁，回滚到这些语句还没开始执行的状态即可，没必要完全回滚，并且与此同时让其他事务继续执行

- Deadlock Prevention，死锁预防

  前面介绍的处理死锁的策略是通过建图来检测是否已经发生了死锁，并且在已经构成死锁后去解开死锁

  Deadlock Prevention这个策略是去预防死锁，不让死锁发生。先根据时间戳给各个事务优先级，规定越先开始的事务它的优先级越高
  死锁的预防有两个方案：

- - Wait-Die（老的事务等待年轻的事务），高优先级的事务想获取低优先级事务已经拥有了的锁时，那么它将等待低优先级的事务去释放锁；如果低优先级的事务想获取高优先级的事务已经拥有了的锁，那么这个事务直接abort回滚

- - Wound-Wait（年轻的事务等待老的事务），高优先级的事务想获取低优先级的事务已经持有的锁时，持有锁的低优先级事务会abort并且释放锁；低优先级的事务想获取高优先级事务已经持有的锁时，它会等待高优先级事务释放这个锁

- 这两个方案本质上是不让事务之间互相等待，因为事务之间互相等待就有可能死锁，这个相当于破除了死锁的必要条件之环路等待。

- - 还可以从更低层面来破除环路等待，比如给每个共享资源的锁设置编号，所有的事务只允许按照一个方向获取多把锁（比如按照编号从小到大，或者从大到小）
  - 这个相当于是更高的层面，如果两个事务要相互等待，优先级低的事务直接abort回滚，避免了环路等待

- <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230306223634479.png" alt="image-20230306223634479" style="zoom:50%;" />

- 此外还要注意，由于预防死锁被abort了的事务重新开始执行时，它的时间戳（即优先级）不会发生变化，不然就有可能一直因为优先级太低被abort，造成饥饿

## Hierarchical Locking

到目前为止锁探讨的锁的粒度一般都是DBMS中如tuple这种的对象的锁，如果一个事务想修改很多很多个tuple，那么它就要不停地获取/释放tuple的锁，这会带来很大的开销，导致性能变差

因此我们不妨加大锁的粒度，当事务想获取锁时，DBMS可以根据实际情况对锁的粒度进行调整（锁的是attribute还是tuple还是数据库文件里的一个页，还是整个表），从而减少事务需要获取的锁的数量

但锁的粒度也有trade-off，需要在获取/释放锁的开销和并行性之间做权衡

DBMS中锁的粒度层级如下所示：

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230307104736763.png" alt="image-20230307104736763" style="zoom:50%;" />

获取整张表的锁，不仅会造成并行性较低，而且加锁的开销也会很大，**因为如果想获取表锁，需要检查它的全部tuple的锁的情况，只要其中有一个tuple的锁被其他事务持有，那当前事务就暂时不能获取这个table的锁**，如果检查到了最后一个tuple才发现有tuple被其他事务锁住，这便十分低效，尤其是表很大tuple、很多的情况下

意向锁（Intention Locks）的存在可以解决上面的问题：**通过对table这种更高层级的对象加一些标记来表明它是否含有被锁住的tuple**，有了这样的意向标记（它并没有真的锁住table），想获取table的锁的事务就不必逐个检查这个table里的tuple

意向锁有三种

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230307105203523.png" alt="image-20230307105203523" style="zoom:50%;" />

- IS ：table含有的tuple中有被上共享锁的
- IX ：table含有的tuple中有被上排他锁的
- SIX ：table含有的tuple中有被上排他锁的，并且整个table也被上了共享锁

这三种意向锁和排他锁，共享锁一起组成了更加复杂的锁兼容矩阵：下图是针对整个表而言的

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230307105408006.png" alt="image-20230307105408006" style="zoom:50%;" />

如果想对tuple加S/IS锁，那必须先对tuple所在的table加IS锁；如果相对tuple加X/IX/SIX锁，那必须先对其所在的table加IX锁

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230307105625577.png" alt="image-20230307105625577" style="zoom:50%;" />

结合例子理解，

例子1:

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230307105744067.png" alt="image-20230307105744067" style="zoom:50%;" />

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230307105806977.png" alt="image-20230307105806977" style="zoom:50%;" />

IS和IX可以兼容。

例子2:

T1可能是带着一个谓词去寻找符合的tuple，然后更新

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230307105943525.png" alt="image-20230307105943525" style="zoom:50%;" />

对于T1，因为要读整个R表，所以需要给它上S锁，又因为需要更新某些tuple，所以要给这些tuple上X锁，进而需要先给R表上IX锁，因此要给R表上SIX锁。遍历整张表的时候不需要给表中的每个tuple上S锁，直接给整个表上了S锁就可以了

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230307110029816.png" alt="image-20230307110029816" style="zoom:50%;" />

对于T2，点查询需要给某个tuple上S锁，因此需要先给table上IS锁，SIX和IS锁可以兼容

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230307110418171.png" alt="image-20230307110418171" style="zoom:50%;" />

并且由于遵守了严格二阶段锁协议，在T1事务和T2事务结束之前，图中这些锁都不能释放，T3到达时，由于要全表扫描，因此需要给table上S锁，但由于SIX和S不兼容（本质上是IX和S不兼容，内部有tuple正在更新时不能读表），所以它要阻塞等待，直到T1释放它所持有的所有锁

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230307110605436.png" alt="image-20230307110605436" style="zoom:50%;" />

# Lec17-时间戳顺序并发控制

本Lecture继续探索并发控制理论的具体实现，上一个Lecture介绍了二阶段锁这一种实现方式，二阶段锁协议有一个不可避免的缺点：锁的存在会导致并行性的下降，进而影响性能。二阶段锁协议属于一种悲观的并发控制方法：总是假设未来可能出现数据竞争，因此总是给共享的对象上锁

与之相对的也存在一些乐观的并发控制方法，比如说接下来要介绍的基于时间戳顺序的并发控制（Timestamp Ordering），它的基本原理是，**给每个事务一个时间戳，根据事务的时间戳来决定它们的顺序以及出现冲突操作时该如何处理**

如果事务`Ti`的时间戳在`Tj`之前，那么DBMS要保证这两个事务都被提交之后的效果相当于`Ti`先执行，`Ti`执行完了之后`Tj`再执行

那么事务的时间戳是怎么设置的呢？

- 可以通过系统时钟，但有些时候这会出问题，因为系统时钟不是完全精确的，它会每隔一段时间和服务器通信，进行同步，因此在同步的时候就会出现“时间突然被调慢了一分钟”这种情况，这就可能导致系统时间校准之前到达的事务和系统时间校准的之后到达的事务的时间戳顺序发生了混乱
- 由于系统时钟并不完全可靠，因此在很多单节点的数据库中使用Logical Counter（逻辑计数器），第一个到达的事务标记为1号，第二个到达的标记为2号...，通过简单的计数完成时间戳的排列，从而避免系统时钟的跳变
- 对于分布式系统来说，Logical Counter很难校准（因为各个节点依赖于网络的通讯比较慢，如果A节点和B节点相距很远的话，有可能某一时刻到达A节点的事务和到达B节点的事务对应同一个counter，因为它们之间不会立即获取到对方的counter更新）。因此就有了系统时钟和逻辑计数器混合使用的时间戳确定办法，具体的实现方法在本课程中并未提及

## 基础的T/O协议

**Basic Timestamp Ordering(T/O) Protocol**

基础的T/O协议中，事务在读/写对象的时候是不加锁的

数据库中的所有对象（一般就是指tuple这种对象）上面要附带两个时间戳，一个读时间戳（上一次读这个对象的事务的时间戳/事务号），一个写时间戳（上一次写这个对象的事务的时间戳/事务号）

每次读/写数据的时候都要检查时间戳（比较当前事务的时间戳和当前准备操作的对象的时间戳），要求是“不能操作未来的数据”。

事务读的流程：

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230307144112307.png" alt="image-20230307144112307" style="zoom:50%;" />

- 如果发现当前事务（上图中的Ti）的时间戳比要读的对象的**写时间戳**（上次修改它的事务的时间戳）早，这就属于“操作来自未来的数据”了，**相当于发生了WR冲突，也就是脏读，当前事务读到的不是期望的数据，而是被其他事务修改过的数据**，无法满足“执行调度的结果等效于先到达的事务完整地在后到达的事务之前发生”，那么当前的Ti事务就不能再操作这个共享的对象，于是abort，之后系统给这个abort的事务一个新的时间戳，事务重启。
- 否则就是合法的（只要当前事务的时间戳比读对象的写时间戳晚就行了，不关心读对象的**读时间戳**），当前事务可以读共享的对象，读完了之后**更新这个对象的读时间戳**（如果这个对象的读时间戳比当前事务的时间戳早，那就把读时间戳修改成当前事务的时间戳），然后把对象从数据库拷贝一份到事务本地（因为对象可能会在接下来被其他事务修改，以便可重复读）

事务写对象的流程如下：

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230307144829207.png" alt="image-20230307144829207" style="zoom:50%;" />

- 如果当前写事务的时间戳早于对象的读时间戳（**相当于发生了RW冲突，也就是不可重复读**）或者早于对象的写时间戳（**相当于发生了WW冲突，也就是覆盖了其他事务的写入**），那么当前事务abort，之后系统给这个abort的事务一个新的时间戳，事务重启。
- 否则（也就是共享对象的上一次读和写都由该在事务之前的事务完成）就可以进行对共享对象的写操作，并且将其拷贝一份放在本地以便可重复读



<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230307151852191.png" alt="image-20230307151852191" style="zoom:50%;" />

出现abort的情况

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230307152104648.png" alt="image-20230307152104648" style="zoom:50%;" />

这里T1 Abort的原因的是：T1覆盖了未来的新的数据，也就是T1覆盖了T2写入的值。

如果T1的W(A)不写入数据库，而是只写在事务本地，随后的R(A)也只是读本地的值，这样的话，T1就不会覆盖T2写入的值，这个执行调度的结果就和串行化后的结果是一样的，T1就不用abort，这个优化方法就是托马斯写规则

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230307160146715.png" alt="image-20230307160146715" style="zoom:50%;" />

- 如果当前写事务的时间戳早于对象的读时间戳，还是和之前一样，直接abort当前事务
- 如果当前写事务的时间戳早于对象的写时间戳，这就表明未来有事务对该共享对象进行修改，那么当前事务不对数据库中的这个对象进行写操作也没问题（因为就算当前事务写了也要被未来的事务覆盖），而是可以直接对保存在事务本地的该对象进行修改，以后该事务要读该对象也是读取保存在事务本地的对象。

基础T/O协议会生成冲突可串行化的执行调度，它的优点是没有采用锁，因此不可能构成死锁，但也有一个问题：较长的事务（比如说有几百条SQL语句）有可能会饥饿，因为很有可能它执行了一段时间之后，想要访问的数据都是被比它更“年轻”的事物修改过的，那它只能abort，重启之后又迎来同样的结局...

基础的T/O协议还有一些性能问题，事务在读任何数据的时候都要往本地拷贝一份，这会带来不小的开销（比如说全表扫描的时候就要把整个表复制一份）

## OCC

**Optimistic Concurrency Control**

此方法也是基于时间戳的，OCC是基于事务之间发生冲突的概率低，并且事务都比较短这个假设提出的优化策略，它希望通过无锁化来对事务之间无冲突的场景做出优化

OCC策略的概述如上图：每个事务都有一个本地保存临时数据的workspace，**读的所有的数据都要保存在本地，对数据的修改不会写入数据库中，而是只写到本地**。等到事务即将commit的时候，DBMS会将要提交的数据更新和其他事务要提交的更新进行比较，如果它们之间没有冲突的话，DBMS会一次性地把事务所做的所有更新都提交，如果出现了冲突，那么事务会abort

OCC有如下三个阶段：

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230307165652693.png" alt="image-20230307165652693" style="zoom:50%;" />

- 读阶段

  进行SQL语句所描述的读写操作。这个阶段之所以称为读阶段，**是对于数据库来说的它是一直只被读的**：因为OCC策略下，事务想写数据时不会直接写到数据库里，而是写在本地，因此数据库中的数据就是一直在被读。除了写数据之外，事务在数据库中访问的所有数据都需要拷贝到workspace，来保证可重复读

- 校验阶段

  事务执行完了准备提交时，把它完成的数据更新和其他事务相比较。如果校验通过，那么就转至下一个阶段，否则abort并重启事务

  OCC策略下数据库中的对象只有写时间戳，**先进入校验阶段的事务会先拿到较早的时间戳**，在进入校验阶段之前事务都没有自己的时间戳

  当事务调用commit的时候就进入了校验阶段，然后会给它分配一个时间戳，校验时DBMS要保证serializable的原则：执行调度的结果等效于时间戳早的事务在时间戳晚的事务之前串行发生，因此它会检查RW冲突与WW冲突，判断相应的Dependency Graph有没有成环

  OCC校验阶段的具体实现方式有两种

  - 向后校验

    即向已经发生过的事务校验。对于下图事务2来说，它走到commit这一步的时候，需要向已经commit过的事务校验，因此会判断事务2和事务1之间的冲突操作是否成环，如果成环了的话，那么事物2 abort

    <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230307171949212.png" alt="image-20230307171949212" style="zoom:50%;" />

  - 向前校验

    即向未来的事务校验。事务2提交的时候，事务3还没有执行完，因此只会校验事务2和事务3有重叠的部分

    <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230307172115088.png" alt="image-20230307172115088" style="zoom:50%;" />

    如果校验的那一部分中出现了成环的冲突操作，由于事务2和事务3（即当前要commit的事务和未来要commit的事务）还都没commit，因此可以根据情况选择它们中的一个来abort

- 写阶段

  把事务本地所记录的更新提交到数据库中

  实现OCC的DBMS一般会在写阶段锁全表，除了当前事务以外没有其他线程可以修改数据库，也就不能多个事务并发地写，虽然这牺牲了并发性，但由于前面已经准备好了要写的数据，所以写操作的时间并不长，因此开销可以接受

## 隔离级别

**Isolation Levels**

我们之前讨论的写都是修改现存的数据，前面的两种并发控制的方法能够解决这种情况，但是对于Insert和Delete这种插入新的数据或删除数据的情况会出现问题：

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230307182927176.png" alt="image-20230307182927176" style="zoom:50%;" />

这就是幻读，第二次读的时候读到了第一次读的时候不存在的东西，或者说**第二次读和之前读的tuple数量不一致**

之所以二阶段锁和OCC这些都预防不了幻读，是因为二阶段锁只是给已经存在了的对象（比如说tuple）上了锁，其他策略也都是只考虑已经存在的对象。也不能给整张表都加写锁，这样会造成并发性低下，性能较低

幻读问题有如下三种解决方案

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230307183224516.png" alt="image-20230307183224516" style="zoom:50%;" />

- Re-Execute Scans

  记录下来事务所有可能出现幻读的地方（像查最大值，平均值，最小值这些涉及到范围扫描的操作），为了防止察觉不到有其他事务在扫描完成后再向表里插入新数据，在事务提交之前会再执行一遍前面所记录的所有扫描

- Predicate Locking谓词锁

  <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230307184801195.png" alt="image-20230307184801195" style="zoom:50%;" />

  对于含有where clause的SQL语句：给select语句对应的谓词加共享锁，给update/insert/delete语句对应的谓词加独占锁

  <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230307184837228.png" alt="image-20230307184837228" style="zoom:50%;" />

- Index Locking

  如果对谓词里的attribute已经构建了索引的话，那么DBMS可以给相关的索引页上锁

  如果没有构建相关的索引，就使用表锁这种宽范围的锁 

  MySQL为了防止幻读实现了间隙锁：MySQL把索引里的内容分成了数据和间隙，也就是说表里同一列连续的数据之间存在间隙，在进行范围扫描的时候会同时锁数据和间隙，这样的话间隙里就不能插入数据（比如说事务里前面有关于最大值的查询，那么就可以在这个查询结束之后把索引中最大值后面的间隙锁住，这样就不会插入更大的值），这和索引锁类似

前面从2PL到T/O这些策略都是为了实现串行化这样一个最高的事务隔离级别，实际的业务场景里有些业务可以忍受非串行化的执行调度，而且非串行化时性能会更好，因此也存在一些比串行化更弱的隔离级别

更低的隔离级别下事务之间会互相暴露，这也会引发一些问题：

- 脏读（当前事务读的数据是其他事务修改过的但这个修改还没被提交）
- 不可重复读（前后两次读同一个对象得到的数据不同，因为被其他事务修改过了）
- 幻读（前后两次读同一个谓词下的数据集合，**读到的数据规模不一样**），insert和delete导致同一个范围扫描的结果不同

![image-20230317203003733](https://raw.githubusercontent.com/BoL0150/image2/master/image-20230317203003733.png)

事务间的各种隔离级别如下，从上到下隔离级别越来越低，但性能也越来越好

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230307185756475.png" alt="image-20230307185756475" style="zoom:50%;" />

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230307190229650.png" alt="image-20230307190229650" style="zoom:50%;" />

读未提交：写锁遵循严格的二阶段锁协议，读的时候不加读锁，无法解决任何问题

读已提交：写锁遵循严格的二阶段锁协议，读的时候加读锁，但是不遵循二阶段锁协议，读完数据后马上可以释放，随后又可以再次获取。可以解决脏读

- 写锁一直到commit之前才会释放，而其他事务读的时候要加读锁，从而其他事务无法读取到已修改未提交的数据

- 如果写锁是不严格的二阶段锁协议，写完之后就释放，则其他的事务就可以读该数据，发生脏读
- 如果读的时候不用加读锁，即便写锁采取了严格的二阶段锁协议，一个事务在修改数据时，其他事务随时可以读此数据，会发生脏读

可重复读：写锁遵循严格的二阶段锁协议，读锁遵循二阶段锁协议，释放后不能再获取，可以解决脏读和不可重复读

- 由于读锁也采用二阶段锁协议，当前事务释放了读锁后就进入了shrinking阶段，不能再获取读锁，也就不能再重复读此数据，从而解决了不可重复读

可序列化：读写锁都遵循严格的二阶段锁协议，并且加索引锁（表级锁），可以再解决幻读

所以**前三级隔离级别的区别在于共享锁的强度，从不加读锁，到加普通的读锁再到按照s2pl协议加读锁**，而写锁统一都是严格的二阶段锁

# Lec18-多版本并发控制

很少有DBMS单独拿MVCC来实现并发控制，MVCC更多地是和2PL，T/O，OCC这些结合起来使用，以达到增强的效果

多版本是指：对于数据库中的每一个对象DBMS可以记录它的多个版本，像git一样

- 当一个事务对一个对象进行修改时，DBMS创建该对象的一个新版本
- 当事务读取一个对象时，读取该对象在事务开始时最新的版本

2PL协议中，一个事务更新了一个对象之后，其他的事务就没有办法读这个对象了，直到这个事务提交；而MVCC的基础思想是，留下数据的历史版本，所以

- 写该数据的事务不会阻塞其他事务读取该数据的历史版本
- 读该数据的事务不会阻塞其他事务修改该数据，因为其他事务不会修改原始数据，只会新增一个版本，即该数据修改后的快照

MVCC给数据库中的对象都记录了它的多个版本后，只读的事务就可以在无锁的情况下读它所需要的那个版本的一致性快照，不受数据库动态变化的影响，而且DBMS一般使用事务的时间戳来决定版本号。有了MVCC之后也可以实现回溯之前的数据

MVCC的实现里也会维护一个全局的事务状态表，便于事务之间互相查询对方的状态（下图Txn Status Table）：

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230307224213487.png" alt="image-20230307224213487" style="zoom:50%;" />

开始时版本库中只有一个A的版本，begin是0 。T1和T2的时间戳分别是1和2。当T2写A的时候，就创建了一个新的版本，该版本的begin是T2的时间戳2，A的前一个版本的end也截止到2 。当T1第二次读A时，就根据T1的时间戳去版本库中找[Begin,end]中包含1的版本。因为MVCC记录了历史版本，所以上图的两个事务得以无锁地进行下来

再看一个例子：

# Lec19-数据库日志

由于我们的dbms采用延迟写回策略，也就是对数据的修改只修改内存中的缓存池，不会立即写回磁盘，所以如果发生crash，就会导致数据的丢失

而我们的dbms要保证以下的要求：

- 任何事务对dbms做的修改在事务**commit成功后**都要是持久化的（注意，是commit之后才能持久化）
- 事务abort后该事务之前进行的任何修改都不能被持久化

![image-20230411155413705](https://raw.githubusercontent.com/BoL0150/image2/master/image-20230411155413705.png)

恢复算法包括两部分：

1. 在正常的事务处理过程中要做哪些操作来保证dbms可以在failure后恢复
2. dbms发生了failure后需要做哪些操作来恢复dbms的状态，使得dbms可以满足原子性，一致性和持久性

本lec主要内容是1

## Failure Classification

DBMS的不同组件依赖于不同的存储设备，比如说缓存池是放在RAM这样的易失性存储上的，数据库里的数据本身是在硬盘这样的非易失性存储器上。

存储设备有如下的分类

![image-20230411161004162](https://raw.githubusercontent.com/BoL0150/image2/master/image-20230411161004162.png)

Stable Storage是一种理想中的存储器：永远不会坏的非易失性存储，只存在于分析问题时的假设中

DBMS中的故障分为如下三级：

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230411161153653.png" alt="image-20230411161153653" style="zoom:50%;" />

第一级，事务级别的故障

- 这又分为逻辑错误（比如说用户要求回滚或者OCC的校验阶段发现不可串行而被迫回滚）和内部状态错误（比如说两个事务之间构成死锁，不得不把其中的一个回滚）

  事务级别的故障是数据库正常运行所不可避免的，因此这也是数据库开发者所必须考虑的

第二级，系统级的故障

- 这又分为软件异常和硬件异常，软件异常可能是DBMS或者OS的bug，硬件故障可能是断电，CPU烧坏了这种，但不包括硬盘故障，硬盘的故障属于第三级故障

第三级，存储介质的故障

- 这类故障一般是无法修复的，数据库开发者无需考虑这些

## Buffer Pool Policies

DBMS需要特定的缓冲池管理方案来实现Undo和Redo操作

undo和redo操作都是在宕机重启后，为了保证DBMS的事务的原子性和持久化对磁盘做的操作

- Undo：crash之后重启时将一个不完整的事务所进行的修改移除，或者将一个abort的事务回滚，保证原子性。
- Redo：crash之后重启恢复时，重放日志，将一个已提交的事务所进行的修改重新安装到磁盘中，保证持久性

如果DBMS要求事务在commit的时候必须把它所做的更新（即脏页）写入磁盘（由于对磁盘读写的单位是页，所以要把更新所在的页一起写回磁盘），就是force策略，否则就是No-Force策略

- force策略是针对脏页来说的，不针对日志

如果DBMS要求事务在commit的时候还要把脏页中**另一个事务已修改未提交的数据**连带着一起写回磁盘，就是steal策略，否则就是No-Steal策略。	

- 如果采用steal策略，那么如果另一个未提交要回滚，就需要把磁盘上的数据一起回滚，因为此时数据已经写回了磁盘
- 如果不采用steal策略，那么在commit时，会开一个新的页，这个页中只包含当前事务自己的数据更新，把这个页刷到磁盘上

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230411170505442.png" alt="image-20230411170505442" style="zoom:50%;" />

No-Steal+Force这个策略的优点是它很容易实现，

- 因为采用Force策略，事务一提交就马上刷盘，已经完成了持久化，所以宕机重启的时候不会存在已提交但是数据没有持久化的事务，所以不需要redo
- 因为采用no-steal策略，未提交的事务所做的更新没有被其他事务连带着写回磁盘，回滚的时候不需要对磁盘进行undo操作，只需要把缓存池改成原来的状态即可

缺点：

- 每次commit都要刷盘，刷盘操作过于频繁，性能不佳
- 由于此策略要求在commit之前不能将任何修改落盘，所以事务在commit之前修改过的所有页都要暂存在内存中，不能被提前驱逐出缓冲池，否则会破坏事务的原子性。特别是全表扫描然后更新的这种场景下，缓冲池就会非常紧张，也就是说，每个事务可修改的数据的量严重受缓冲池大小的的限制

## Shadow Paging

Shadow Paging是上文No-Steal+Force策略的一个具体实现

具体略

Shadow Paging策略会造成对硬盘的大量的随机访问，这会降低性能，DBMS需要一个方案去把对磁盘的随机的写转换成连续的写，这就是下面要介绍的WAL（Write-Ahead Log）

## Write-Ahead Log

WAL，翻译成中文是“预写日志”，也就是要先写日志，再写数据。在这个策略里，磁盘里面会单独开辟一块类似于前面所讲的journal file的区域，称为log file，用于保存事务对于数据的修改。为了便于分析问题，我们假设日志文件在可靠的不容易坏的存储设备上存储（不考虑此存储设备存到一半crash的情况），日志里面有足够多的信息让DBMS完成Undo/Redo操作

DBMS在把用户所修改过的缓存池中的页刷入磁盘之前，它需要先把预写日志写入磁盘中的日志文件，也就是说先完成预写日志的持久化，再完成缓存池里dirty page的持久化

wal通常是steal + no-force的缓存池策略（注意，wal是no-force，事务提交时脏页不需要落盘，只有日志需要落盘。如果采用force，那么wal就没有意义了）

WAL协议的具体内容如下：

修改数据库文件的数据页的时候，自然要把修改先写入缓存池，**日志也会先被写在内存里**，内存里一般会开辟一个专用的缓存专门用来写日志。**在将数据页落盘前，所有的日志记录必须先落盘**，在日志落盘之后，事务就被认为已经提交成功（其实此时数据还没有持久化到磁盘中，但是只要日志落盘了，就可以向用户保证数据是安全的）。

日志中往往会插入如下的记录：事务刚开始的时候会写一个begin record，事务提交的时候会写一个commit record，这两个record之间记录的就是事务所做的全部操作，当一个事务提交时，不会像之前force策略一样，直接把缓冲池中的脏页刷盘，而是仅仅把内存日志在begin和commit之间的记录全部被刷入硬盘的日志，再把“提交成功”的消息返回给上层的应用程序或者用户

WAL策略下，日志的每一条entry包含如下的内容：

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230411202749195.png" alt="image-20230411202749195" style="zoom:50%;" />

object是被修改的数据库中的对象的ID，before value就是这个对象的数据被修改之前的值，它是Undo操作的时候要用到的；after value就是这个对象的数据被修改后的值，它是Redo操作的时候要用到的

- 由于wal对脏页采用延迟写回策略，commit时不会立即将脏页落盘，所以crash重启时，磁盘中只有数据的日志，没有数据，此时需要使用磁盘中的wal，对磁盘进行redo

MySQL的write ahead log被分成了两部分，分为undo log和redo log，当然在很多工业界DBMS的实现中，还是把这两种log合并成了一个log，去集中管理全部的日志。MySQL把WAL分成undo/redo log的好处是 undo log可以单独拿出来用来做delta storage（这属于MVCC相关的内容），回推出来数据的上一个版本，从而实现MVCC，并且把undo/redo log分开存储也更安全。MySQL中，undo log和数据库中的数据本身被存储在一个文件里（不和数据库的数据在同一个页里），redo log是被存储在单独的文件中的。

结合如下例子理解WAL：

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230412105147078.png" alt="image-20230412105147078" style="zoom:50%;" />

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230412105209094.png" alt="image-20230412105209094" style="zoom:50%;" />

具体实现：

WAL协议的日志什么时候落盘？

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230412111234864.png" alt="image-20230412111234864" style="zoom:50%;" />

WAL协议规定要在事务commit的时候把它的日志写入硬盘，在实际的DBMS实现当中，有一个优化-group commit，“组提交”：在数据库系统中，磁盘I/O的开销是非常巨大的，如果每提交一个事务就要进行一系列的磁盘I/O，那么性能不是太好。不妨让用户多等待一会儿，把多个事务的日志攒到一起提交（第一个到达的事务执行到commit时先阻塞住，然后多攒几个要commit的事务，之后一次性地把它们的日志全部刷入磁盘，然后同时将“事务成功提交”返回给所有等待着的用户），**这样的话就会把多个事务的磁盘I/O合并到一起来处理，减少磁盘I/O的次数与开销**。（xv6的日志系统就是使用的组提交，一次性地提交多个和文件系统有关的系统调用的事务日志，并且在执行相关的系统调用而陷入内核时先做检查，检查当前攒批的所有事务所要写的日志的大小是否已经超过日志容量的上限，如果没有的话就可以执行本次系统调用，否则就要等待这波攒批提交完之后再继续执行系统调用）

- 优化磁盘IO的开销只有一种办法，采用延迟刷盘，把多次刷盘攒成一次刷盘，既可以减少磁盘IO次数，也可以把随机写入尽量变成顺序写入。

  比如缓冲池就采用延迟刷盘策略，而为了保证事务的原子性和持久性，还需要采用wal，将脏页的落盘转化为日志的落盘，也就是将随机访问磁盘变成了顺序访问磁盘。而如上所述，wal也采用延迟刷盘策略，把多个事务的日志提交攒在一起落盘。

日志文件可以有很多个页，如果日志的buffer满了的话可以让后台的线程把部分日志页写入磁盘，这不会太影响当前正在执行的事务的性能

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230412110713740.png" alt="image-20230412110713740" style="zoom:50%;" />

如果某段时间内DBMS中的事务数量不多，很长的一段时间内都没有新的事务到达，那么也没有必要等到事务攒够一定的数量再group commit，不然会导致等待的时间过长。可以设置一个等待时长上限，到了这个上限之后就把当前所有想要commit的事务的日志写入硬盘。

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230412110834315.png" alt="image-20230412110834315" style="zoom:50%;" />

WAL协议的脏数据什么时候落盘？

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230412111608282.png" alt="image-20230412111608282" style="zoom:50%;" />

之前的force策略是事务每次commit时都要将脏数据落盘，更有甚者，每次事务对数据进行了一次更新都要落盘。所以这些策略的性能都很差。

我们从两个方面对性能进行分析：

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230412115759662.png" alt="image-20230412115759662" style="zoom:50%;" />

- runtime performance是事务正常执行的时候的性能，no-force+steal策略的runtime performance是最好的，因为steal策略会把多次的刷dirty page回磁盘的I/O合并成一次，no-force策略下没有强制要求用户在commit的时候刷盘；与之相反，force+no-steal的runtime performance最差。

- recover performance是数据库恢复的时候的性能。恢复性能和运行性能恰好是相反的，no-force+steal策略下的recover performance最差，因为no-force策略会有大量数据没有刷盘，导致宕机后重启时用log重放事务；steal策略会导致往磁盘里刷了好多没有还没有提交的事务所做的数据更新，因此重启时DBMS需要执行undo操作把crash的时候没执行完的事务回滚。

  与之对应，force+no-steal的恢复性能最好，正如我们之前所提到的，由于force策略每次事务commit都会将数据落盘，所以宕机重启后不需要执行redo；no-steal策略从来不会将未提交的数据落盘，所以宕机重启之后也不需要执行undo。

**但是DBMS的开发者们更在意它的运行性能，进而选择no-force+steal的缓存池策略，因为crash后恢复的场景是相对比较罕见的，可能DBMS运行几年，几个月才有一次宕机恢复的情况发生**。



## Logging Schemes

接下来将深入分析WAL（write ahead log）的格式，也就是WAL所记录的具体内容

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230412120316473.png" alt="image-20230412120316473" style="zoom:50%;" />

- 物理日志：记录每一个页具体到二进制级别的变化（e.g. 某个页的xxx偏移量位置处发生了xxx变化）
- 逻辑日志：记录事务所执行的操作，比如说事务所执行的SQL语句

物理日志的缺点是，如果事务所执行的操作是对很多很多的数据进行修改（比如说修改全表的某个attribute的值），那么物理日志将会非常庞大，**如果用逻辑日志来记录，要记录的可能仅仅是一条SQL语句**

正如前面所说，逻辑日志的优点是所记录的数据的量比物理日志要少。但逻辑日志也有其缺点：

- 如果逻辑日志是记录事务所执行的SQL语句，那么如果事务执行了“记录数据库当前的时间”这样的SQL指令（可能会通过SQL的`now`函数来实现），在redo的时候就会出现问题。也就是说，如果只采用逻辑日志，在redo重放日志的时候可能会重放出来错误的结果，或者说是出现逻辑问题。
- 逻辑日志在宕机重启redo的时候开销比较大，需要重新执行sql语句

现实中，通常使用的物理日志是***phsilogical logging***

这个策略大体上和之前的物理日志差不多，唯一的区别是记录的是某个页的第几号槽（slot）指向的tuple内的数据的变化而非在偏移量xxx处的变化，这就不会受存储引擎整理数据页中的tuple时造成的影响（删除tuple后会导致页中出现空穴和碎片，因此DBMS会整理tuple，在整理前后，同一个tuple在这个页中的偏移量会发生变化，因此就在清理碎片的同时需要维护物理日志，如果日志里记录的是slot号而非偏移量，那么就省去了这个维护操作带来的开销）

phsilogical logging是常见的WAL的实现方式，MySQL的官方文档中所说的物理日志指的就是这个

但是phsilogical logging还是没有解决“修改的数据太多导致日志过于庞大”的问题，因此在此基础上还有一些改进的策略：MySQL中的Mixed Logging，在这个策略中，如果DBMS检测到*phsilogical logging*日志太长，并且经过分析可以得出只记逻辑日志的话在redo时不会出现问题（也就是不涉及前面所提及的那些特殊情况），那么可以混合地记日志：一会儿记*phsilogical logging*日志，一会儿记逻辑日志，这样的话就更好地结合了各种日志策略的优点，但也在前面判断是否可以记混合日志这个阶段有一点额外的开销。

而且*phsilogical logging*相比于物理日志，在redo/回放时会多一些开销，因为需要根据slot号推断出具体的偏移量是多少。

逻辑计划在redo的时候开销最大，因为需要把SQL语句重新执行

如下所示的SQL语句会导致在表以及对应的索引结构里修改数据，与之对应的各种日志形式如下图所示（**这里是把undo log和redo log写到一起了。before用于undo，after用于redo，MySQL是把undo和redo log分开的**

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230412171955749.png" alt="image-20230412171955749" style="zoom:50%;" />

 

## Checkpoints

这个和单机游戏中的“存档点”很类似

磁盘中的日志如果不清理的话它会越来越大，把磁盘占满。并且数据库崩溃的时候如果不知道该从哪个点开始恢复，就会造成类似于“玩游戏从来不存档”的后果。因此DBMS会在日志里面加一些存档点-'checkpoint'，有了存档点之后，可以把存档点之前的日志都删掉，并且崩溃恢复的时候，只需要回放存档点之后的日志即可

DBMS会定期地把全部的**脏页和日志**（包括日志也要刷进去）都被写入磁盘（未提交的事务的脏页也要被刷盘吗？由于wal采用steal+no-force的策略，所以加checkpoint的时候应该是把所有的脏页都刷盘的），然后在日志中打上checkpoint。也就是说，**日志中checkpoint说明在它以上的所有日志，以及这些日志所关联 的脏页已经全都被刷进了磁盘**。 

宕机重启恢复时，**在checkpoint之前就提交了的事务不用管**。而在checkpoint之前，T2和T3都没有commit，因此要对它们做redo或undo操作

- T2在crash之前commit了，所以重启时要对T2进行redo
- T3在crash之前没有commit，所以重启时要对T3进行undo

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230412174001018.png" alt="image-20230412174001018" style="zoom:50%;" />

Checkpoint在实际实现当中存在一些问题:

- 只有当DBMS中已有的事务都结束，而且新的事务没有开始，也就是数据库中没有任何事务在进行，整个数据库是静态的时候，在这样的一个“空档期”才能做checkpoint，这样才能保证实现一致性的快照
- Checkpoint本身肯定会带来性能损耗，存档需要花时间做磁盘I/O，事务的执行会被阻塞住。

- 数据库恢复的时候需要去根据日志判断哪些事务在crash前没有commit，进而需要扫描日志文件，因此也会带来开销。
- 数据库多长时间存档一次也是一个玄学问题，如果存档过于频繁，性能会变差，如果存档频率极低，会导致每次存档时，有太多东西要写入磁盘，如果事务运行了一半的时候DBMS开始了存档，那么事务就会被阻塞很长时间。并且如果存档的频率过低，在数据库恢复的时候也比较麻烦，因为会有好多日志要回放。

## 优化磁盘IO

以上都是为了优化磁盘IO，而优化磁盘IO的开销只有一种办法，采用延迟刷盘，把多次刷盘攒成一次刷盘，既可以减少磁盘IO次数，也可以把随机写入尽量变成顺序写入。

比如缓冲池就采用延迟刷盘策略，即事务提交时不将脏页落盘，而为了保证事务的原子性和持久性，还需要采用wal，事务每次提交时只用将日志落盘，不用将脏页落盘，也就是将脏页落盘时的随机读写变成了添加日志的顺序读写。而如上所述，wal也采用延迟刷盘策略，把多个事务的日志提交攒在一起落盘。

# Lec20-数据库恢复

恢复算法包括两部分：

1. 在正常的事务处理过程中要做哪些操作来保证dbms可以在failure后恢复
2. dbms发生了failure后需要做哪些操作来恢复dbms的状态，使得dbms可以满足原子性，一致性和持久性

上一节是关于1，这一节的内容是关于2

本Lecture将介绍ARIES算法，

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230413182219823.png" alt="image-20230413182219823" style="zoom:50%;" />

翻译成中文是“数据库恢复原型算法”，这个算法是用来做数据库恢复的，但只是一个原型，提供了一个基本的理念，工业界中各个数据库基于这个理念给出了各种不同的实现

ARIES算法主要的想法如下：

- 使用wal，事务在提交的时候需要把日志刷到磁盘上，数据页可以不用刷盘
- 刷脏页的时候缓存池策略采用steal+No-force，即刷脏页的时候把同一页上其他事务未提交的修改也一起刷盘
- 在DBMS宕机重启之后，按照日志的内容进行重放（redo）
- 在 undo 过程中记录 undo 操作到日志中，确保在恢复期间再次出现故障时不会执行多次相同的 undo 操作

## Log Sequence Numbers

WAL中的每条**日志记录**都需要包含一个全局唯一且一般是单调递增的log sequence number (LSN)。而DBMS中的不同部分都需要记录某些相关的LSN信息

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230413184134455.png" alt="image-20230413184134455" style="zoom:50%;" />

- flushedLSN：记录在内存中，整个DBMS维护一个全局的flushedLSN；表示最后落盘的那个日志的LSN，用于记录现在有哪些日志记录已经被刷入磁盘
- pageLSN：每一个数据页都有一个pageLSN，代表着对这个页进行最近一次修改的SQL语句对应的日志的LSN，也就是记录的是缓存当中这个页最新的修改
- recLSN：也是在数据页里面，用于记录自从这个页上次被刷入磁盘之后第一个修改这个页的SQL语句对应的日志的LSN，即内存里面这个页最老的修改，**pageLSN和recLSN是缓存里面对于这个页的修改的上限（最新的修改）和下限（最早的修改）**
- lastLSN：在事务内部，用于记录到目前为止该事务留下的最后一条日志的 LSN
- MasterRecord：维护在磁盘中，表示上一次打的checkpoint点对应的LSN

事务每一次修改缓存里面的数据页，都要顺带修改pageLSN，每次把内存里的log刷入磁盘时也要顺带更新flushedLSN

由于wal要求日志比数据页先落盘，所以将脏页刷盘时，要保证修改过这个脏页的所有操作所对应的所有日志记录已经被刷入磁盘，并且比这些日志记录的LSN小的日志记录也都要被刷进磁盘，才能将这个脏页刷盘

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230413185750782.png" alt="image-20230413185750782" style="zoom:50%;" />

如果pageLSN大于flushedLSN的话，就说明修改该页的日志还没有落盘，此时不能将这个脏页落盘

如：

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230413190821945.png" alt="image-20230413190821945" style="zoom:50%;" />

下图的缓冲池中的页的pageLSN小于flushedLSN，表明修改该页的日志已经落盘了，所以可以将该页落盘

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230413190845337.png" alt="image-20230413190845337" style="zoom:50%;" />

下图中的该页的日志明显还未落盘，所以不能将该页落盘

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230413191042129.png" alt="image-20230413191042129" style="zoom:50%;" />

## Normal Commit & Abort Operations

**在事务正常地commit与回滚的时候要做的操作**

为了方便研究问题，我们首先做如下的假设：

- 所有log在一个页中
- 往磁盘中刷记录时是原子的（实际不是这样，如果DBMS的数据页是16k，而磁盘的数据页是4k，刷盘时就不是原子的）
- 事务用s2pl协议
- 用steal+no-force的wal缓冲池管理协议

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230413195721507.png" alt="image-20230413195721507" style="zoom:50%;" />

事务提交时要向日志中写入COMMIT记录

事务commit时向磁盘中写入日志的过程是连续的写（因为就是在磁盘中的日志文件后面不断地追加），而且是同步的（synchronous writes to disk，线程会一直阻塞，直至把日志记录全部写入磁盘，全部写入后会返回给用户commit成功）。再将flushedLSN修改为COMMIT记录的LSN

事务提交之后，只有日志落盘了，脏页并没有落盘。**此时在用户看来此事务已经结束了，但是在DBMS看来此事务还没有结束**。只有当该事务的脏页也都完成了落盘（比如当发生了一个checkpoint时），在DBMS看来此事务才算完全成功，此时会再写一条'TXN-END'这样的日志记录，但它不需要马上落盘

也就是说：

- commit只意味着该事务的日志已经落盘了
- txn-end才意味着该事务的数据已经落盘了

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230413201134860.png" alt="image-20230413201134860" style="zoom:50%;" />

当日志落盘之后，在内存中的这些日志就可以删除了（即小于flushedLSN的日志）

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230413201334126.png" alt="image-20230413201334126"  />



前面介绍了事务在正常commit的时候所做的操作，接下来介绍事务在回滚的时候所做的操作：

**这里的回滚是事务正常的回滚，不是宕机重启后进行undo的回滚**。

**当事务abort时，也要像commit一样，向日志中写入abort记录**

ARIES算法会对回滚做特殊的处理：在日志记录中再加一个字段：prevLSN，加在每一条日志记录的后面，它代表着**在该事务中**，这一条日志的上一条日志的LSN（e.g. 15号日志记录的上一条日志记录不一定是14号日志记录，**因为它们可能属于不同的事务**），这就**方便事务回滚的时候找到它的所有日志**

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230413202046063.png" alt="image-20230413202046063" style="zoom:50%;" />

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230413202354361.png" alt="image-20230413202354361"  />

如上图所示，我们在回滚的时候就可以从日志中的Abort记录开始，沿着prevLSN倒着对每一条日志进行undo（恢复日志中维护的旧的值）。除此之外，在对事务进行undo的时候，要记录"compensation log records"（简称CLR）。对Abort之前的日志每做一次undo操作，就要在Abort之后创建一条对应的CLR日志，CLR日志中记录了这次undo具体的操作。CLR中还记录了undoNext指针，该指针指向下一个将要被undo的日志的LSN。

**CLR的目的是防止在回滚过程中再次故障导致部分操作被执行多次。有了CLR，回滚的过程中如果发生宕机重启，只需要找到最新的CLR，从该CLR指向的日志开始回滚即可**。

等待所有操作回滚完毕后，DBMS 再往 WAL 中写入 TXN-END 记录，意味着所有与这个事务有关的日志和数据都已经写完（因为此事务已经回滚完了，就像没发生过一样，此事务对系统的所有修改都消失了，所以此事务没有数据需要落盘），不会再出现相关信息。

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230413211957119.png" alt="image-20230413211957119" style="zoom:50%;" />

CLR日志不需要强制落盘，因为大不了下次DBMS重启的时候重新回滚一遍，不影响事务的原子性，所以实际上**即使没有CLR日志也不影响DBMS的功能**。**在事务abort的时候可以直接告知用户事务abort了，而不必等到所有的undo操作都做完并且CLR都落盘**

## Fuzzy Checkpointing

模糊检查点

**Fuzzy Checkpointing是对之前介绍的checkpoint机制的优化**

在之前所介绍的checkpoint实现方法中，DBMS在标记checkpoint的时候整个DBMS都要是静止的，不能有任何新的事务开始执行，正在执行的事务也要等到它们执行结束。如果在还没执行完的那些事务里，有的事务非常的长，那么DBMS就会等待相当长的一段时间，在这段时间里不能接收新的事务，这会使得性能非常的差

- 但是这种做法也有好处，crash recovery的时候非常方便。在crash recovery的时候，checkpoint之前的所有事务所做的数据更新都已经落盘，因此数据库恢复的时候完全不用看checkpoint之前的日志记录，checkpoint之前的日志记录可以直接被删掉。

解决方案是：加检查点时，对要刷盘的数据加上写锁，不让其他的事务去修改这些数据

这时**有些未commit的事务写入的数据可能会被checkpoint线程一起捎带落盘**，因此这时磁盘中的数据 snapshot可能处于 inconsistent 的状态。但是只要我们在 checkpoint 的时候记录哪些活跃事务正在进行，哪些数据页是脏的，故障恢复时读取 WAL 就能知道存在哪些活跃事务的数据可能被部分写出，从而恢复 inconsistent 的数据。因此整个 checkpoint 过程需要两类信息：活跃事务表 (ATT)与脏页表(DPT)

- ATT中的每个entry代表着进行checkpoint的时候仍活跃的事务，entry中的内容包括如下所示的三项：

  <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230413215854535.png" alt="image-20230413215854535" style="zoom:50%;" />

  事务彻底完成的时候（也就是它所做的全部更新都落盘，即TXN-END被写入日志时），ATT表中它所对应的entry才可以被删除

  （事务状态中的U可以理解为“还没提交”：如果数据库crash的时候这个事务还没提交，那么它就需要undo，所以称为"candidate for undo"）

- DPT记录的是缓存池中当前还没落盘的脏页，每一个entry代表一个未落盘的脏页，entry中有这个脏页的recLSN（第一个让这个页变脏的日志的LSN）

# 分布式数据库

## system architecture

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230429173705161.png" alt="image-20230429173705161" style="zoom:50%;" />

共享内存：此种架构在分布式中应用极少，因为内存最大的优势就是CPU访问很快，如果CPU需要走网络才能访问内存，那么内存存在的意义就没有了。但是在非分布式世界中，是有应用的。比如有所谓的4路服务器，就是有四个CPU（不是四个核，每个CPU中有好几块核），CPU之间通过高速总线相连，比如超级计算机就是内部有几百上千颗CPU。

共享磁盘：每个节点有各自的CPU和内存，真正的数据存储共享的磁盘中，CPU通过网络访问磁盘。所有的CPU就好像在访问一个单块的磁盘，不需要关心存储集群是如何实现的，实际存储集群内部可能由很多台机器组成。应用服务器向计算节点发送请求，计算节点再通过网络从存储节点中获取相应的page，拉到内存中进行处理，将应用请求的数据返回给应用服务器。

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230429181121863.png" alt="image-20230429181121863" style="zoom:50%;" /> 

特点在于：

- 此架构中上面的计算节点和下面的存储节点是可以解耦的，如果计算能力不够可以单独扩充计算节点，同样，也可以单独扩充存储节点。

  <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230429181238408.png" alt="image-20230429181238408" style="zoom:50%;" />

- 需要解决缓存一致性的问题：如果应用向一个服务器发出了修改请求，这个服务器修改的是它的缓存里的数据，还没有同步到磁盘中。如果应用又向另一个服务器发出读请求，这个服务器向磁盘拉取数据，或者读取它自己内存中的旧数据，就会导致出错。**只要在不同的CPU之间不共享缓存**就会导致缓存一致性问题，比如在多核CPU内部，每个核都会有各自的cache，此时也会出现缓存一致性的问题。

  <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230429181905700.png" alt="image-20230429181905700" style="zoom:50%;" />

此架构在近些年越来越流行（polarDB），因为现在流行将公司的数据库部署在云机房上，比如阿里云，腾讯云等，而不是本地的机房。云机房在设计时天然就是计算节点和存储节点分离的。

**什么都不共享（shared nothing）**：每个节点都有各自的CPU，内存和磁盘，节点之间通过网络通信。**由于不同节点之间磁盘都不共享，所以此架构需要进行数据分区，将数据库的数据分散在不同的节点的磁盘上**，应用服务器请求不同的数据需要到不同的节点请求，如果应用请求的数据在某台服务器上没有，此服务器还要负责转发请求到有这条数据的节点，帮应用找数据。

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230429182934229.png" alt="image-20230429182934229" style="zoom:50%;" />

特点：

- 耦合性太强，难以扩容，想单独增加计算能力和单独增加存储能力都做不到。并且由于数据是分散在不同节点上的，所以扩容和收缩时还需要将数据重新分布

  <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230429183309309.png" alt="image-20230429183309309" style="zoom:50%;" />

- 想要做到数据一致性更加困难

- 如果一个节点挂了，那么该节点磁盘中的数据就无法访问了，就相当于整个数据库都废了，所以此架构crash的概率更高，为了解决此问题，我们需要想办法把数据复制到多台机器上，这样一台机器挂了，我们还有其他的机器上的副本，从而保证单机失效的情况下，数据不丢失，不出错。

  一个 Region 的数据会保存多个副本，我们将每一个副本叫做一个 Replica。Replica 之间是通过 Raft 来保持数据的一致，**一个 Region 的多个 Replica 会保存在不同的节点上**，构成一个 Raft Group。其中一个 Replica 会作为这个 Group 的 Leader，其他的 Replica 作为 Follower。**所有的读和写都是通过 Leader 进行，再由 Leader 复制给 Follower**。 

  <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230429184231725.png" alt="image-20230429184231725" style="zoom:50%;" />

- 优点是性能更好，访问磁盘的速度更快

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230429182426635.png" alt="image-20230429182426635" style="zoom:50%;" />

数据量变多了之后，一台机器放不下，就出现了分布式数据库来解决问题。

## design issues

分布式系统带来的问题：

- 应用如何知道哪个数据在哪个节点？
- 如何在分布式的数据上执行查询？
  - 方法一：在不同的节点上分别执行查询，再将查询的结果汇总
  - 方法二：将数据汇总到一个节点上，在该节点上执行查询
- 如何保证一致性

<img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230429202535037.png" alt="image-20230429202535037" style="zoom:50%;" />

节点的类型分为：

1. 集群中所有节点的职责都是一样的，只是数据不一样

2. 集群中每个节点的职责不一样，比如mongodb，config server负责给其他的节点发送数据的位置

   <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230429203009918.png" alt="image-20230429203009918" style="zoom:50%;" />

数据透明：分布式数据库应该要保证，用户访问分布式数据库应该要像在访问一个单节点的数据库一样，用户不需要关心数据实际存储在哪个节点中，以及一张表是怎么分区或备份的

数据分区：

- naive方法：将一整张表放在一个单独的节点（前提是每个节点有足够的空间放一整张表），如果从来不对不同的表执行join查询，那么此法就比较理想，否则执行join查询效率很差。

  <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230429205749014.png" alt="image-20230429205749014" style="zoom:50%;" />

- 水平分区：

  将一张表水平切开，以tuple为单位分区。对tuple中的某列进行hash，将tuple分到不同的节点中。此法的问题在于：如果点查询时谓词中没有指明partitionKey的值，那么需要在所有的节点中都搜索一遍。比如：如果一条select的谓词是日期，那么就要在下图的所有节点中都搜索一遍

  <img src="https://raw.githubusercontent.com/BoL0150/image2/master/image-20230429210013368.png" alt="image-20230429210013368" style="zoom:50%;" />
